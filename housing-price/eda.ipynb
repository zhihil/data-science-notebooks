{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJIsnZy0Dmxk"
      },
      "source": [
        "# Predicting Housing Prices, Exploratory Data Analysis\n",
        "## Overview\n",
        "\n",
        "The dataset is a Housing Prices dataset found [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) \n",
        "\n",
        "We will build a regression model to predict housing prices and submit to the competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_IEb2ukDrYK"
      },
      "source": [
        "## Basic Exploration\n",
        "\n",
        "We begin with a shallow investigation of the features. The goals are to:\n",
        "1. Determine which features are numeric and which are categorical\n",
        "2. Determine if any features are missing\n",
        "\n",
        "Using the above information, we will perform the bare minimum data preprocessing to get a linear regression model working. This baseline model will be the benchmark we will improve our models against.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmINF7be0VR9",
        "outputId": "245b1f22-1af6-49db-cb1a-719290b369a0"
      },
      "source": [
        "!pip install -U scikit-learn\n",
        "!pip install -U statsmodels\n",
        "!pip show scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n",
            "Collecting statsmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/69/8eef30a6237c54f3c0b524140e2975f4b1eea3489b45eb3339574fc8acee/statsmodels-0.12.2-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=1.1 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->statsmodels) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5->statsmodels) (1.15.0)\n",
            "Installing collected packages: statsmodels\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed statsmodels-0.12.2\n",
            "Name: scikit-learn\n",
            "Version: 0.24.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: numpy, scipy, joblib, threadpoolctl\n",
            "Required-by: yellowbrick, textgenrnn, sklearn, sklearn-pandas, mlxtend, lightgbm, librosa, imbalanced-learn, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "kx0BO7l5DxGh",
        "outputId": "d3753916-dec0-4c02-dfd6-0aa8f0c7897f"
      },
      "source": [
        "# Mount so that we can access files on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-85296ae4307d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount so that we can access files on Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o3Hfz4cGVrH"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Colab Notebooks/housing-prices/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY1qfQZzGmlD"
      },
      "source": [
        "# Loading the data\n",
        "import pandas as pd\n",
        "\n",
        "workdir = \"/content/drive/My Drive/Colab Notebooks/housing-prices/\"\n",
        "train_filepath = workdir + \"train.csv\"\n",
        "test_filepath = workdir + \"test.csv\"\n",
        "\n",
        "raw_data = pd.read_csv(train_filepath, index_col=\"Id\")\n",
        "X_test = pd.read_csv(test_filepath, index_col=\"Id\")\n",
        "\n",
        "target_col = 'SalePrice'\n",
        "X = raw_data.drop(target_col, axis=1)\n",
        "y = raw_data[target_col]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ-IBijHHYU8"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cciwB9nIf9jG"
      },
      "source": [
        "X.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZH0reLEHcqV"
      },
      "source": [
        "X.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtA5JWUrIg_n"
      },
      "source": [
        "# Determine then numerical features from the categorical features\n",
        "mask = X.dtypes == 'object'\n",
        "object_cols = list(mask[mask].index)\n",
        "numerical_cols = list(set(X.columns) - set(object_cols))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIPXI5BPgqqE"
      },
      "source": [
        "object_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iTWIX4hgra7"
      },
      "source": [
        "numerical_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK8De6kjJS41"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "global_random_state = 0\n",
        "global_valid_size = 0.3\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=global_valid_size, random_state=global_random_state\n",
        ")\n",
        "\n",
        "# Creating combined dataset in-case it is needed for EDA later.\n",
        "combined_train = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
        "combined_valid = pd.merge(X_valid, y_valid, left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHfUQ4SHLntW"
      },
      "source": [
        "null_count = X_train.isnull().sum(axis=0)\n",
        "with_null = null_count[null_count > 0].index\n",
        "with_null_types = X_train[with_null.tolist()].dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHIvo5ieMiuA"
      },
      "source": [
        "The above EDA suggests the following minimum preprocessing to get a model working:\n",
        "1. Encoding categorical features (we will use one-hot encoding for the baseline)\n",
        "2. Feature scaling numerical features (we will use standardization)\n",
        "3. Imputing numerical features (we will use mean imputation for the baseline)\n",
        "4. Imputing categorical features (we will use the most frequent value for the baseline) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fslmji9ND2U3"
      },
      "source": [
        "## Constructing a Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwplBRTLEI9b"
      },
      "source": [
        "### Baseline Linear Regression Model\n",
        "1. Construct a simple model\n",
        "2. Perform the bare minimum data preprocessing necessary for the regression\n",
        "3. Evaluate the model\n",
        "4. Plot the learning curve\n",
        "5. Visualize the linear regression if possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLYevFjaLS8l"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "numerical_transformer = Pipeline(\n",
        "    steps=[('num_impute', SimpleImputer(strategy='mean')),\n",
        "           ('standardize', StandardScaler())]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[('cat_impute', SimpleImputer(strategy='most_frequent')),\n",
        "           ('ord_encoding', OneHotEncoder(handle_unknown=\"ignore\"))]\n",
        ")\n",
        "\n",
        "# Using a subset of the features because using all 79 features will cause an explosion in MSE\n",
        "baseline_num_cols = ['YearBuilt', 'OverallQual', 'OverallCond', 'LotArea', '1stFlrSF', \n",
        "                     '2ndFlrSF', 'BedroomAbvGr', 'WoodDeckSF', 'OpenPorchSF']\n",
        "baseline_cat_cols = ['LotFrontage', 'KitchenQual', 'Heating', 'RoofStyle', 'RoofMatl',\n",
        "                     'BldgType', 'Neighborhood']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', numerical_transformer, baseline_num_cols),\n",
        "                  ('cat', categorical_transformer, baseline_cat_cols)]\n",
        ")\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                       ('model', model)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNat7s4YPaeo"
      },
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, \n",
        "                        train_sizes=np.linspace(.1, 1.0, 5), scoring=None):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, \n",
        "                                                            n_jobs=n_jobs,\n",
        "                                                            train_sizes=train_sizes, \n",
        "                                                            scoring=scoring)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                     color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    \n",
        "    return train_scores, test_scores\n",
        "\n",
        "def pretty_learning_curve(clf, X, y, scoring, train_sizes):\n",
        "  \n",
        "  plt.xlabel(\"Dataset Size\")\n",
        "  plt.ylabel(\"Score\")\n",
        "  train_scores, valid_scores = plot_learning_curve(clf, \"Learning Curve\", X, y, \n",
        "                                                   train_sizes=train_sizes, scoring=scoring)\n",
        "  plt.show()\n",
        "\n",
        "  for train_size, scores in zip(train_sizes, train_scores):\n",
        "    print(\"Train score - size %3d - mean %10f - scores %60s\" % \n",
        "          (train_size, np.mean(scores), str(scores)))\n",
        "\n",
        "  print()\n",
        "\n",
        "  for train_size, scores in zip(train_sizes, valid_scores):\n",
        "    print(\"Valid score - size %3d - mean %10f - scores %60s\" % \n",
        "          (train_size, np.mean(scores), str(scores)))\n",
        "\n",
        "X_baseline = X[baseline_num_cols + baseline_cat_cols]\n",
        "y_baseline = y\n",
        "\n",
        "train_sizes = [100 * i + 1 for i in range(0, 12)]\n",
        "pretty_learning_curve(pipe, X_baseline, y_baseline, \n",
        "                      scoring=make_scorer(mean_squared_error), \n",
        "                      train_sizes=train_sizes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZVmUhmpjRDg"
      },
      "source": [
        "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def score_classifier(pipe, X, y, cv_num=10, n_repeats=5):\n",
        "  cv_strategy = RepeatedKFold(n_splits=cv_num, n_repeats=n_repeats, \n",
        "                              random_state=global_random_state)\n",
        "  scoring_strategy = make_scorer(mean_squared_error)\n",
        "  score = cross_val_score(pipe, X, y, cv=cv_strategy, \n",
        "                          n_jobs=-1, scoring=scoring_strategy)\n",
        "  return np.mean(score), score\n",
        "\n",
        "class Evaluator():\n",
        "  def fit(self, pipe, X, y):\n",
        "    self.baseline = score_classifier(pipe, X, y)[0]\n",
        "    return self\n",
        "\n",
        "  def evaluate(self, pipe, X, y):\n",
        "    return score_classifier(pipe, X, y)[0] - self.baseline\n",
        "\n",
        "  def get_baseline(self):\n",
        "    return self.baseline\n",
        "\n",
        "evaluator = Evaluator()\n",
        "evaluator.fit(pipe, X_baseline, y_baseline)\n",
        "\n",
        "print(\"The baseline cross-validation score is: %f\" % \n",
        "      evaluator.get_baseline()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPocfscgF3Fj"
      },
      "source": [
        "# # Uncomment for submission to Kaggle\n",
        "# pipe.fit(X, y)\n",
        "# baseline_preds = pipe.predict(X_test)\n",
        "\n",
        "# output = pd.DataFrame({'Id': X_test.index, 'SalePrice': baseline_preds})\n",
        "# output.to_csv(workdir + '/submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0802PMi8HBK2"
      },
      "source": [
        "### Results\n",
        "Kaggle provides a sample submissions in `submission.csv` which has a score of 0.40613. \n",
        "\n",
        "The baseline model we submit has a score of about 0.17987 on Kaggle, so this is the target we will beat.\n",
        "\n",
        "As a proxy for the Kaggle score, we will beat the cross-validation score of: 1248912305.301590, when working on this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIGP2x3KELa7"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "The model is below the expected performance. Therefore, we will consider the following to improve model performance:\n",
        "\n",
        "1. Check the assumptions of linear regression and manipulate the data to meet the assumptions if possible.\n",
        "2. Try alternative models such as Lasso, ElasticNet, RidgeRegression, and linear SVM. \n",
        "\n",
        "There might be overfitting, as the validation score has a gap with the training score. Although graphically this gap appears small, the cross-validation score is infact twice that of the training score. Ideally, the two scores should be as close as possible. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKfr3npxD8OT"
      },
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "Our goals are:\n",
        "1. Check feature assumptions\n",
        "2. Check model assumptions \n",
        "3. Determine the best kind of categorical encoding for each column\n",
        "4. Determine a good imputation strategy for each column\n",
        "5. Produce descriptive statistics and note any interest patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il040tivFett"
      },
      "source": [
        "### Feature Documentation\n",
        "\n",
        "Below, we've surveyed all the features listed in `data_description.txt` obtained from [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). We classify the type of feature, categorize them if they seem related, estimate the importance of the feature in predicting the target based on intuition and some domain research, and then suggest data preprocessing steps. \n",
        "\n",
        "| Variable        | Feature Type                             | Category                     | Expected Predictive Weight on Target | Description                                                  | Proposed Preprocessing                                       |\n",
        "| --------------- | ---------------------------------------- | ---------------------------- | ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| `SalePrice`     | Continuous                               | Target                       | N/A                                  | The property's sale price in dollars.                        | N/A                                                          |\n",
        "| `MSSubClass`    | Nominal [15]                             | Building, Qualitative        | High                                 | The type of dwelling gives building layout information, such as the number of stories, or a duplex building vs. fmaily conversion. Intuitively, 2-story should be more expensive than 1-story, so this may play a large role in `SalePrice`` | Needs more sophsiticated categorical encoding                |\n",
        "| `MSZoning`      | Nominal [8]                              | Neighborhood                 | High                                 | Distinguishes zones such as `Agriculture` vs. `Commercial` vs. `Industrial`, etc. More details [here](https://www.cityofames.org/government/departments-divisions-i-z/legal/city-of-ames-municipal-code/zoning-table-of-contents-municipal-code-chapter-29).<br />Certain areas might be more desirable than others, e.g, if `Industrial` zones allow manufacturing plants, then nearby residences might have lower property value due to the unappealing industrial centers nearby. | Needs more sophsiticated categorical encoding                |\n",
        "| `LotFrontage`   | Continuous (I)                           | Lot, Quantitative            | Medium                               | The linear feet of street connected to property. It is better illustrated [here](https://stats.stackexchange.com/questions/189678/is-it-better-to-do-exploratory-data-analysis-on-the-training-dataset-only). <br /><br />This might not be a significant variable. Intuitively, the influence of `LotFrontage` might be \"logarithmic\" in sense that a large frontage is not very valuable, but a small frontage could be inconvenient. | Standardization                                              |\n",
        "| `LotArea`       | Continuous (I)                           | Lot, Quantitative            | Medium                               | The lot size in square feet. <br />Lot area might raise costs due to the inherent value of the land, however, for a homebuyer, it might not be a priority. | Standardization, no feature engineering (already in square ft.),  `SalePrice` is possibly linear with this feature |\n",
        "| `Street`        | Nominal [2]                              | Building, Qualitative        | Low                                  | Type of road access to property: Gravel or Paved. Unlikely to be a significant factor for buyers. | Binary encoding should suffice                               |\n",
        "| `Alley`         | Nominal [3]                              | Building, Qualitative        | Low                                  | Type of alley access to property: Gravel, Paved, or NA. Unlikely to be a significant factor for buyters. | One-hot encoding, drop if insignificant                      |\n",
        "| `LotShape`      | Nominal [4]                              | Lot, Qualitative             | Medium                               | See [here](https://www.gimme-shelter.com/lot-shape-50068/). Seems to interact with `LotArea` and `LotFrontage`. According to the article, having a high `LotArea` is meaningless if the `LotShape` is concentrated on the front lot, which is worth less than the back lot. However, the area concentrated on the front lot could be inferred by a combination of `LotArea` and `LotShape`, so `LotShape` might be redundant | One-hot encoding, possibly redundant with `LotFrontage` and `LotArea`? |\n",
        "| `LandContour`   | Nominal [4]                              | Land, Qualitative            | Low                                  | Includes level, banked, hillside, or depression. Unless the contour is distorted to the point of impractically, this isn't something significant that people consider. | One-hot encoding                                             |\n",
        "| `Utilities`     | Nominal [4]                              | Building, Qualitative        | High                                 | This is one of the first concerns when buying a house. This is probably a significant influence on the `SalePrice` | One-hot encoding                                             |\n",
        "| `LotConfig`     | Nominal [5]                              | Lot, Qualitative             | Low                                  | Includes `Inside lot`, `Corner lot`, and `CulDSac`. This does not seem to be a major factor for a buyer's choice | One-hot encoding                                             |\n",
        "| `LandSlope`     | Ordinal [3]                              | Land, Qualitative            | Medium                               | This might be a somewhat influential factor in `SalePrice`. A severe slope might be inconvenient and drive down prices. | One-hot encoding                                             |\n",
        "| `Neighborhood`  | Nominal [25]                             | Neighborhood                 | High                                 | This might be a significant factor. A neighborhood implies a lot of information: the quality of public education in the area, the crime rate of the neighborhood, and distance to travel the work. Independently of what the other features are, buyers might have subjective bias for particular neighborhoods. | Needs more sophisticated feature engineering. There are too many categories for one-hot encoding to be practical. Perhaps use target encoding and rank them by average `SalePrice`? The `Neighborhood` might nonlinearly influence all of the other features. Individuals of different social status may gravitate to different neighborhoods. Consequently, expensive renovations might drive up `SalePrice` in wealthier neighborhoods, moreso than lower class neighborhoods, where buyers would focus on the necessities over luxuries. |\n",
        "| `Condition1`    | Nominal [9]                              | Neighborhood                 | Medium                               | Proximity to railroads or density-packed streets might reduced property value due to noise pollution. Proximity to parks might increase property value, since the neighborhood is perceived to be desirable or appealing. | Needs more sophisticated categorical encoding                |\n",
        "| `Condition1`    | Nominal [9]                              | Neighborhood                 | Medium                               | Same as above                                                | Needs more sophisticated categorical encoding                |\n",
        "| `BldgType`      | Nominal [5]                              | Building, Qualitative        | High                                 | Includes types such as: \"Single-family detached\" or \"Townhouse End Unit.\" The type of housing influence how much space is included, which will impact the price. | One-hot encoding                                             |\n",
        "| `HouseStyle`    | Nominal [8]                              | Building, Qualitative        | High                                 | Includes the number of floors such as 1 story, 1.5 story, 2 story, and whether the additional stories are finished or not. | Needs more sophisticated categorical encoding                |\n",
        "| `OverallQual`   | Ordinal [10]                             | Building, Qualitative        | High                                 | General equality of the house is probably an influential factor in `SalePrice`. No-one wants to live in a home of shoddy make. | Ordinal encoding, standardization, might be nonlinear with `SalePrice` |\n",
        "| `YearBuilt`     | Continuous (I)                           | Building, Quantitative       | Medium                               | The age of a house might contribute to its `SalePrice` , since the quality may decrease over time, which may lead to decrease in `SalePrice`. | Standardization                                              |\n",
        "| `YearRemodAdd`  | Continuous (I)                           | Building, Quantitative       | Medium                               | Remodel date (same as construction date if no remodeling or additions). This might offset any depreciation of prices due to an old `YearBuilt` | Standardization, perhaps some feature engineering with `YearBuilt` or dropping `YearBuilt` entirely? |\n",
        "| `RoofStyle`     | Nominal [6]                              | Building, Qualitative        | High                                 | Type of the roof. Roof shape and material might play a large role, because they will determine what it is like to live in the house. For example, glass walls have poor insulation. The make and type of roof might similarly affect the quality of living in the house. | Needs more sophisticated categorical encoding                |\n",
        "| `RoofMaterial`  | Nominal [8]                              | Building, Qualitative        | High                                 | Material of the roof. Same as above.                         | Needs more sophisticated categorical encoding                |\n",
        "| `Exterior1st`   | Nominal [17]                             | Building, Qualitative        | High                                 | The exterior material of the house might strongly impact the `SalePrice`, because: (1) the material used contributes the house's visual appeal and may impact `SalePrice`, (2) the materials used may affect the quality of living in the house, e.g, one of the values is \"Asbestos Shingles\" and if asbestos material is damaged and becomes airborne, it carries significant health risks. | Needs more sophisticated categorical encoding                |\n",
        "| `Exterior2st`   | Nominal [17]                             | Building, Qualitative        | High                                 | Same as above                                                | Needs more sophisticated categorical encoding, feature engineering (possibly add a boolean indicator that a second exterior exists?) |\n",
        "| `MasVnrType`    | Nominal [5]                              | Building, Qualitative        | High?                                | Masonry veneer appears to refer to an additional outer wall, according to [here](https://en.wikipedia.org/wiki/Masonry_veneer). <br />This feature appears to have the same consequences as `Exterior1st` and `Exterior2st`. | One-hot encoding                                             |\n",
        "| `MasVnrArea`    | Continuous (I)                           | Building, Quantitative       | Low                                  | The area of the veneer might not be a factor that buyers typically focus on. | Standardization,  `SalePrice` is possibly linear with this feature |\n",
        "| `ExterQual`     | Ordinal                                  | Building, Qualitative        | High                                 | The quality of the material, intuitively, will contribute to the `SalePrice`. People will likely pay less for houses that are  lower quality. | Ordinal encoding, standardization, feature engineering (`SalePrice` might not be a linear funciton of `ExteralQual`) |\n",
        "| `ExterCond`     | Ordinal                                  | Building, Qualitative        | High                                 | The quality of the material, intuitively, will contribute to the `SalePrice`. People will likely pay less for houses that are  lower quality. | Ordinal encoding, standardization, feature engineering (`SalePrice` might not be a linear funciton of `ExteralCond`) |\n",
        "| `Foundation`    | Nominal [6]                              | Building, Qualitative        | Low                                  | The `Foundation` might objectively contribute the quality of living in the home, but this does not seem like something that a buyer typically considers because it is not visible, so because of the buyer's subjective perception, it does not influence the `SalePrice` | Needs more sophisticated categorical encoding                |\n",
        "| `BsmtQual`      | Ordinal                                  | Basement, Qualitative        | Medium                               | A basement does not seem necessary. Therefore, it might be seen as a bonus for buyers. Therefore, `SalePrice` would not be strongly influenced by this feature, because a \"luxury\" like a basement wouldn't justify a significant increase in price. | Ordinal encoding, standardization, might be nonlinear with `SalePrice` |\n",
        "| `BsmtCond`      | Ordinal                                  | Basement, Qualitative        | Medium                               | Same as above.                                               | Ordinal encoding, standardization, might be nonlinear with `SalePrice` |\n",
        "| `BsmtExposure`  | Ordinal                                  | Basement, Qualitative        | Low                                  | Refers to \"walkout\" or garden level walls. Concretely, this refers to a door from the basement to outside the house. This seems like a very low-priority asset that would not significantly influence `SalePrice` | Ordinal encoding, standardization, consider dropping this feature |\n",
        "| `BsmtFinType1`  | Nominal [7], could consider as Ordinal   | Basement, Qualitative        | Medium                               | Refers to the quality of the basement: good living quarters, average living quarters, etc. | Ordinal encoding, standardization, `SalePrice` may be a nonlinear function of this feature |\n",
        "| `BsmtFinSF1`    | Continuous (I)                           | Basement, Quantitative       | Medium                               | The surface area of the basement. Intuitively, a larger basement would influence the `SalePrice` | Standardization, `SalePrice` is possibly linear with this feature |\n",
        "| `BsmtFinType2`  | Nominal [7], could consider as Ordinal   | Basement, Qualitative        | Medium                               | Same as `BsmtFinType1`                                       | Ordinal encoding, standardization, `SalePrice` may be a nonlinear function of this feature, feature engineering (perhaps add a boolean indicating a second finish type exists?) |\n",
        "| `BsmtFinSF2`    | Continuous (I)                           | Basement, Quantitative       | Medium                               | Same as `BsmtFinSF1`                                         | Standardization, `SalePrice` is possibly linear with this feature, (feature engineering; two possibilties: (1) combine `BsmtFinSF1` and `BsmtFinSF2` to produce new total sf, (2) the price per unit surface area might depend on `BsmtFinType` so account for the value of the finish type) |\n",
        "| `TotalBsmtSF`   | Continuous (I)                           | Basement, Quantitative       | Medium                               | Same as `BsmtFinSF1`                                         | Standardization, `SalePrice` is possibly linear with this feature, consider replacing `BsmtFinSF1` and `BsmtFinSF2` with this feature. The `FinSF` features are multicollinear and redundant. |\n",
        "| `Heating`       | Nominal [6]                              | Heating, Qualitative         | High                                 | The value of this feature depends on the climate of the city. The data comes from Ames, Iowa, US, which has a \"humid continental climate\" according to [Wikipedia](https://en.wikipedia.org/wiki/Iowa). The city of Des Moines is close to Iowa, so we can infer the monthly normal highs/lows are similar. The winter temperature is relatively comparable to Toronto, possibly more severe, so good heating would be desirable. Hence, the feature might have a high influence on `SalePrice` | Needs more sophisticated categorical encoding. We could perform ordinal encoding if we can rank the heating by some measure (consider target encoding). |\n",
        "| `HeatingQC`     | Ordinal                                  | Heating, Qualitative         | High                                 | Based on the reasoning above, the quality of heating might be crucial. | Ordinal encoding, standardization, might be nonlinear with `SalePrice`, might need to be used in tandem with `Heating` |\n",
        "| `CentralAir`    | Nominal [2]                              | AC, Qualitative              | High                                 | From [this](https://fischerheating.com/decentralized-vs-centralized-hvac-system/) article, there are a number of considerations:<br />1. The choice of an HVAC system depends on several factors, so `CentralAir` should be used in tandem with building surface area<br />2. A centralized HVAC can be aesthetically pleasing to buyers, because the machinery is hidden behind the building.<br />3. Centralized systems have a high up-front cost compared to decentralized systems.<br />4. A decentralized system might have a lower lifespan and might induce maintenance costs.<br />The exact effect of `CentralAir` on `SalePrice` is unclear, but it is very likely that `CentralAir` has some impact. | Binary encoding                                              |\n",
        "| `Electrical`    | Nominal [5], some ordinal ordering       | Electrical, Qualitative      | High                                 | The quality of the electrical system is intuitively one of the first things a buyer will consider when buying a house. | One-hot encoding                                             |\n",
        "| `1stFlrSF`      | Continuous (I)                           | Building, Main, Quantitative | High                                 | Greater first floor surface area might imply a greater amount of land used. The raw value of land might drive up `SalePrice` | Standardization, possibly linear with `SalePrice`            |\n",
        "| `2ndFlrSF`      | Continuous (I)                           | Building, Main, Quantitative | High                                 | More surface area on the second floor implies additional costs to construct the house, driving up `SalePrice`. It is likely that `2ndFlrSF <= 1stFlrSF` so the raw value of the land will have been accounted for by `1stFlrSF`. This implies that the second floor might be priced at a different rate compared to the first floor. | Standardization, possibly linear with `SalePrice`, consider summing this with `1stFlrSF`. On the other hand, different floors might have different pricing rates. |\n",
        "| `LowQualFinSF`  | Continuous (I)                           | Building, Main, Quantitative | High                                 | If the amount of low qualified surface feet is significant, then this might make a negative impression to buyers and drive the `SalePrice` | Standardization. Possibly a non-linear relationship with `SalePrice`? A small amount of low qualify finish might be forgivable, but a large patch might turn off buyers, driving down the `SalePrice` |\n",
        "| `GrLivArea`     | Continuous (I)                           | Building, Main, Quantitative | High                                 | Above grade living area might drive up the price.            | Standardization.                                             |\n",
        "| `BsmtFullBath`  | Discrete                                 | Building, Main, Quantitative | Low                                  | Additional bathrooms might be convenient, but might not justify a markup in `SalePrice`. Buyers will likely be looking for bathrooms that look OK. As suggested by [this](https://www.opendoor.com/w/blog/improvements-that-increase-home-value) website, it seems that bathrooms will have less of an impact on `SalePrice` than other renovations. | Standardization                                              |\n",
        "| `BsmtHalfBath`  | Discrete                                 | Building, Main, Quantitative | Low                                  | Same reasoning as above, but according to the Opendoor website, a half bathroom might be worth even less when marking up `SalePrice` | Standardization                                              |\n",
        "| `FullBath`      | Discrete                                 | Building, Main, Quantitative | Medium                               | Same reasoning as above. The quality matters a bit more than the number of bathrooms, but it is possible that a fancy bathroom won't lead to a significant markup in `SalePrice` | Standardization                                              |\n",
        "| `HalfBath`      | Discrete                                 | Building, Main, Quantitative | Medium                               | Same reasoning as above                                      | Standardization                                              |\n",
        "| `Bedroom`       | Discrete                                 | Building, Main, Quantitative | High                                 | According to [this](https://www.opendoor.com/w/blog/improvements-that-increase-home-value) source, adding a 3rd bedroom results in a good increase in sale value. A bedroom might be more valuable than a `Bedroom` which it comes to increasing `SalePrice` | Standardization                                              |\n",
        "| `Kitchen`       | N/A                                      | Missing feature              | N/A                                  | This feature is not actually included with the `.csv`        | N/A                                                          |\n",
        "| `KitchenQual`   | Ordinal                                  | Building, Main, Quantitative | High                                 | According to [this](https://www.opendoor.com/w/blog/improvements-that-increase-home-value) source, remodelling a kitchen results in a good increase in sale value. | Ordinal encoding, standardization, might be nonlinear with `SalePrice` |\n",
        "| `TotRmsAbvGrd`  | Discrete                                 | Building, Main, Quantitative | High                                 | `TotRmsAbvGrid` might give some information about the overall quality of the property, as opposed to one or two rooms such as the bathroom. The rationale is that, individually, a high quality room might not significantly impact `SalePrice`, but their cumulative value could drive up prices. | Standardization                                              |\n",
        "| `Functional`    | Ordinal                                  | Building, Main, Qualitative  | (Very?) High                         | Having a functioning home without major issues will likely play a significant role into `SalePrice`. Some minor deductions could seriously drive down `SalePrice`. Major deductions could cause steep declines in `SalePrice` | Imputation (the data description states to asssume `Typ` unless deductions warranted, this suggests the imputation strategy should use `Typ` for missing values) |\n",
        "| `Fireplaces`    | Discrete                                 | Building, Main, Quantitative | Low                                  | This seems like a luxury feature that wouldn't justify a significant increase `SalePrice`. Having 1 fireplace might be nice, but having 2 fireplaces is unnecessary. | Standardization                                              |\n",
        "| `FireplaceQu`   | Ordinal                                  | Building, Main, Quantitative | Low                                  | Same reasoning as above. Might not significantly drive up `SalePrice`. | Ordinal encoding, standardization, might be nonlinear with `SalePrice` |\n",
        "| `GarageType`    | Nominal [6]                              | Garage, Qualitative          | Low                                  | Where the garage is positioned doesn't seem like it would matter at all. | One-hot encoding                                             |\n",
        "| `GarageYrBuilt` | Continuous [i]                           | Garage, Quantitative         | Low                                  | The year in which the garage was built might not matter, unless it strongly influences the quality of the garage. | Standardization                                              |\n",
        "| `GarageFinish`  | Nominal [4], could be considered ordinal | Garage, Qualitative          | Medium                               | A high quality garage might be somewhat valuable, because a garage can act as a workshop or play area. However, it might not be as valuable as the \"main\" rooms of a house such as the kitchen or bedroom. | Ordinal encoding, standadization, possibly nonlinear         |\n",
        "| `GarageCars`    | Discrete                                 | Garage, Quantitative         | Medium                               | Same reasoning as `GarageArea`                               | Standardization, possibly redundant with `GarageArea`?       |\n",
        "| `GarageArea`    | Continuous (I)                           | Garage, Quantitative         | Medium                               | A garage consumes a significant amount of space. If we count purely the land value, then this might have a noticeable impact on `SalePrice` | Standardization, possibly linear with `SalePrice`            |\n",
        "| `GarageQual`    | Ordinal                                  | Garage, Qualitative          | Medium                               | Quality of the garage likely impacts the `SalePrice`         | Ordinal encoding, standardization, possibly nonlinear.       |\n",
        "| `GarageCond`    | Ordinal                                  | Garage, Qualitative          | Medum                                | Condition of the garage likely impacts the `SalePrice`       | Ordinal encoding, standardization, possibly nonlinear.       |\n",
        "| `PavedDrive`    | Nominal, can be interpreted as Ordinal   | Garage, Qualitative          | Low                                  | This does not seem to be a influential factor in a home's overall quality. | Ordinal encoding, standardization, possibly nonlinear.       |\n",
        "| `WoodDeckSF`    | Continuous (I)                           | Outdoor, Quantitative        | Low                                  | A wood deck might have some influence, but does not seem to be factor. | Standardization                                              |\n",
        "| `OpenPorchSF`   | Continuous (I)                           | Outdoor, Quantitative        | Medium                               | An open porch might be a somewhat valuable luxury feature, but it is unlikely that it contributes significantly to `SalePrice` | Standardization                                              |\n",
        "| `EnclosedPorch` | Continuous (I)                           | Outdoor, Quantitative        | Medium                               | The enclosed porch area in squar e feet. Same reasoning as above. Might contribute slightly more to `SalePrice` due to the cost of the enclosure. | Standardization                                              |\n",
        "| `3SsnPorch`     | Continuous (I)                           | Outdoor, Quantitative        | Medium                               | The enclosed porch area in squar e feet. Same reasoning as above. Might contribute slightly more to `SalePrice` due to the cost of the enclosure and furnishing. | Standardization                                              |\n",
        "| `PoolArea`      | Continuous (I)                           | Pool, Quantitative           | Medium                               | Ames weather will have extreme cold, so a pool is not always valuable. However, the raw cost of a pool might nonetheless drive up the `SalePrice` | Standardization                                              |\n",
        "| `PoolQC`        | Ordinal                                  | Pool, Qualitative            | Medium                               | Pool quality is critical if a pool is under consideration by the buyer. A poorly maintained pool could be a hazard and would contribute negatively to `SalePrice` | Ordinal encoding, standardization, nonlinearity, should be used in tan dem with `PoolArea` |\n",
        "| `Fence`         | Ordinal                                  | Main, Qualitative            | High                                 | The fence quality might be an important factor in the `SalePrice`. This asset might be closer to a necessity. | Ordinal encoding, standardization, nonlinearity.             |\n",
        "| `MiscFeature`   | Ordinal                                  | Misc, Qualitative |     Medium | Includes luxury features like a tennis court or elevator, which may significantly drive up prices | \n",
        "| `MiscVal`       | Continuous (I)                           | Misc, Quantitative           | Medium                                    | Luxury features might drive up `SalePrice` | Standardization                                              |\n",
        "| `MoSold`        | Discrete                                 | Sale, Quantitative           | High                                 | This could a meta-feature that nonlinearly influences other features. Financial bubbles, for example, might change the behaviour of buyers, and therefore, the expected influence of other features on `SalePrice` | Standardization; Meta-feature                                |\n",
        "| `YrSold`        | Continuous (I)                           | Sale, Quantitative           | High                                 | Same as above                                                | Standardization; Meta-feature. Possibly combine this with `MoSold` somehow. |\n",
        "| `SaleType`      | Nominal [10]                             | Sale, Qualitative            | High                                 | The type of sale might be influential in `SalePrice` depending on how `SalePrice` is recorded. Assuming `SalePrice` is the upfront payment, then a pure cash sale will report a higher `SalePrice`, than one that has significant interest. | Sophisticated categorical feature engineering;  meta-feature |\n",
        "| `SaleCondition` | Nominal [6]                              | Sale, Qualitative            | High                                 | This might be a significant factor in `SalePrice`. One of the listed values is `Family`. It is possible that a sale between family members is more \"forgiving\" and thus will nonlinearly influence the other features, causing a lower `SalePrice` | Sophisticated categorical feature engineering;  Meta-feature; Feature engineering (features such as `SaleCondition` might have a nonlinear impact on the `SalePrice`, in the sense that, if all other features, $x$, contributes $cx$ to the `SalePrice`, then `SaleCondition` could be a feature that adjusts the value of the rate $c$, which is a nonlinear relationship). The rationale for this is that real-estate agents would want to maximize profit, so the rates $c$ might be driven up, whereas family members might offer less than market pricing out of generosity. |\n",
        "\n",
        "(I) - Technically discrete integers, but effectively continuous for our purposes. \n",
        "\n",
        "- The expected predictive weight for each feature ranges from None, Low, Medium, or Strong.\n",
        "- Generally speaking:\n",
        "  - Necessities like utilities, bedrooms, kitchens, overall quality are probably High\n",
        "  - Luxuries like porches, pools, fancy bathrooms, are probably Medium\n",
        "  - Minor details like the pavement of the driveway are probaby Low.\n",
        "  - Totally irrelevant information is None."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N2dZQPVs6Ih"
      },
      "source": [
        "Note that the feature documentation is **inaccurate**. There are inconsistencies with what features are present in the dataset and what features are present in the documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLZ1HHpUrp4S"
      },
      "source": [
        "# The features listed in `data_description.txt`\n",
        "data_documentation_features = ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'Bedroom', 'Kitchen', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n",
        "\n",
        "# Features in the actual dataset\n",
        "X_features = list(X.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nTVz6Acs5QQ"
      },
      "source": [
        "set(X_features) - set(data_documentation_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SsK3mD0tlu8"
      },
      "source": [
        "Hence, the dataset has the features `BedroomAbvGr` and `KitchenAbvGr` which are not documented. However, since `TotalRmsAbvGrd` is documented and means \"total rooms above grade\", we can infer that `BedroomAbvGr` and `KitchenAbvGr` means bedroom/kitchen above grade, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuhFv5het-Wu"
      },
      "source": [
        "set(data_documentation_features) - set(X_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C71mgMEtgrf"
      },
      "source": [
        "We see that the data documentation describes features called `\"Bedroom\"` and `\"Kitchen\"`. The exact meaning of this feature is somewhat confusing, because the Kaggle description describes the feature with the phrase \"above basement level\" whereas the `data_description.txt` describes the feature with \"above grade.\"\n",
        "\n",
        "This [article](https://www.gimme-shelter.com/below-grade-50067/) suggests that \"above grade\" and \"above basement level\" mean the same thing.\n",
        "\n",
        "This shows that `\"Bedroom\"` and `\"Kitchen\"` are equivalent to `\"BedroomAbvGr\"` and `\"KitchenAbvGr\"`, respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz2KkRkdD-NY"
      },
      "source": [
        "### Univariate Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6CiZHsSghKu"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import scoreatpercentile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kge_d5-_mRE5"
      },
      "source": [
        "#### Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnQfXx6BOwkG"
      },
      "source": [
        "y_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP7WAm2_gxZd"
      },
      "source": [
        "The mean is ~180,000 which is greater than the median which is ~160,000. This suggests a right skewed distribution. \n",
        "\n",
        "50% of the data lies within the range [130000, 215000]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3POE10sMgii_"
      },
      "source": [
        "sns.displot(y_train, kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5y_0UtUlBXK"
      },
      "source": [
        "print(\"Skew: %f\" % y_train.skew())\n",
        "print(\"Kurtosis: %f\" % y_train.kurt())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC4TPDO7lapU"
      },
      "source": [
        "# Normal distribution kurtosis and skew for reference.\n",
        "ex = pd.Series(np.random.normal(size=10000))\n",
        "print(\"Normal skew: %f\" % ex.skew())\n",
        "print(\"Normal kurtosis: %f\" % ex.kurt())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdC5-jwdhJRz"
      },
      "source": [
        "The relative frequency histogram illustrates the right-skew of the `SalePrice`. The skewness and kurtosis further suggests that `SalePrice` does not follow a Normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NogHmmcQhEz7"
      },
      "source": [
        "sns.boxplot(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvZGfZuohIZh"
      },
      "source": [
        "From the boxplot, there appear to be a significant number of outliers which have a higher price than expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4po4TifhjfT"
      },
      "source": [
        "q75 = scoreatpercentile(y_train, 75)\n",
        "q25 = scoreatpercentile(y_train, 25)\n",
        "IQR = q75 - q25\n",
        "outlier_threshold = q75 + 1.5 * IQR\n",
        "outlier_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ3h7cPSiJft"
      },
      "source": [
        "Hence, non-outliers must have a `SalePrice` strictly less than the `outlier_threshold` above "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5JqtCyXiQg1"
      },
      "source": [
        "nonoutliers = (y_train < outlier_threshold).sum()\n",
        "outliers = (y_train >= outlier_threshold).sum()\n",
        "\n",
        "print(\"Non-outliers: %d\" % nonoutliers)\n",
        "print(\"Outliers: %d\" % outliers)\n",
        "print(\"Outliers as percent of overall data: %f\" % (outliers / len(y_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAEAtxdgioLF"
      },
      "source": [
        "Therefore, slightly less than 4% of the data are outliers. \n",
        "\n",
        "- **[ENG]** - Try removing the records correspond to `SalePrice` outliers to see if that improves model performance.\n",
        "- **[TASK]** - Identify any interesting characteristics about the outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEvAEBgWmSX5"
      },
      "source": [
        "#### Predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HOq0vKvF2E9"
      },
      "source": [
        "##### Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCjukF3MmoX8"
      },
      "source": [
        "X_train[numerical_cols].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7nfm7dzqy9w"
      },
      "source": [
        "First, we define \"luxury\" to mean any asset of a house that is uncommon, say 75% of houses do not have it. We define \"high-end\" to mean any asset where the only 75th percentile value or greater indicates a presence of this asset. We define \"average\" to mean assets where the 50th percentile value or greater indicates a presence of this asset. \n",
        "\n",
        "Any feature marked with these labels needs further investigation to confirm if this is the case, and to check if these represent outliers, i.e, lavish, expensive homes. \n",
        "\n",
        "We make the following observations\n",
        "\n",
        "- `2ndFlrSF` - At least 50% of the records have a value of 0 for `2ndFlrSF`. This indicates that having a second floor is not common.\n",
        "- `KitchenAbvGr` - Since the 25th and 75th percentiles have value 1.0. This indicates that most people ($\\ge 50$%) have only 1 kitchen above grade.\n",
        "- `3SsnPorch` - Since the 75th percentile has value 0. This indicates that a significant number of houses do not have a 3-seasons porch. This asset is probably a luxury. \n",
        "- `BedroomAbvGr` - Since the 25th percentile has value 0, most houses have at least 2 bedrooms \"above grade.\" A notable number of the population have at least 3 ($\\ge 25$%). \n",
        "- `MSSubClass` - We categorized this as a nominal variable in the feature documentation. This is incorrectly typed.\n",
        "- `YearRemodAdd` - Most of the housing ($\\ge 50$%) were built in 1967-2003. \n",
        "- `LotFrontage` - There are few homes missing a `LotFrontage`. There are two possibilities: the value is non-zero and simply missing, the value should be zero because technically no street is connected to the lot. In reality, there could be a mix of both possibilities and other interpretations.\n",
        "- `GarageCars` - Since the 25th percentile is 1.0, then 75% of homes have space for at least 1 car, 50% of homes have space for at least 2. \n",
        "- `OpenPorchSF` - At least 50% of homes do not have an open porch. \n",
        "- `LowQualFinSF` - At least 75% of homes do not have low quality finishes. We can infer from a box-and-whisker plot that any homes with `LowQualFinSF` are outliers\n",
        "- `FullBath` - At least 75% of homes have a full bathroom, at least 50% have 2 full bathrooms. \n",
        "- `EnclosedPorch` - At least 75% of homes do not have an enclosed porch. This asset must be a luxury.\n",
        "- `Fireplaces` - At least 50% of homes have a fireplace. This asset might not be as much of a luxury as previously assumed.\n",
        "- `GarageYrBlt` - This has some missing values and requires deeper investigation to determine a proper imputation. \n",
        "- `BsmtUnfSF` - At least 75% of homes have at least 230 square feet of unfinished basement surface area. \n",
        "- `BsmtFinSF2` - At least 75% of homes do not have a second type of basement. We know then that houses with a second type of basement are outliers, possibly luxury homes.\n",
        "- `GarageArea` - At least 25% of homes have at least 325 square feet of garage area.\n",
        "- `OverallQual` - At least 25% of houses have at least material and finish quality of 5/10. At least 75% of more houses have quality at most 7/10. \n",
        "- `OverallCond` - At least 25% of houses have at least an overall condition of 5/10. At least 75% of more houses have condition at most 6/10. This and `OverallQual` is actually rather unsurprising because 5 indicates \"Average\" in the dataset's scale. \n",
        "- `GrLivArea` - 50% of homes have above-grade living area from around 1140 to 1790 square feet.\n",
        "- `BsmtHalfBath` - At least 75% of people do not have a half-bathroom in the basement. This indicates that house that do have one are outliers. This asset is possibly a luxury. \n",
        "- `MiscVal` - At least 75% of homes do not have this miscellaneous value. Whatever it may be. This asset is possibly a luxury. \n",
        "- `LotArea` - At least 25% of homes have a lot area of atleast 7742.5 square feet.\n",
        "- `PoolArea` - At least 75% of homes do not have any pool. This asset seems like a luxury feature. \n",
        "- `ScreenPorch` - At least 50% of homes do not have a screen porch. This asset appears to be a luxury feature. \n",
        "- `MasVnrArea` - At least 50% of homes do not have any masonry veneer. There are some missing values for this feature. This asset might be a high-end feature.\n",
        "- `WoodDeckSF` - At least 50% of homes do not have a wood deck. This asset might be a high-end feature.\n",
        "- `HalfBath` - This is a high-end feature.\n",
        "- `MoSold` - This feature might be better visualized through a barplot.\n",
        "- `BsmtFullBath` - High-end feature\n",
        "- `BsmtFinSF` - Average feature. \n",
        "- `YearBuilt` - 1954-2000 is the construction date for 50% of homes recorded. It may be worthwhile computing the time between the `YearBuilt` and the year sold. \n",
        "\n",
        "The median or \"average\" home has the following characteristics:\n",
        "\n",
        "- No second floor\n",
        "- A total basement surface area of ~992 square feet.\n",
        "- 1 kitchen above grade\n",
        "- 3 bedrooms above grade\n",
        "- A first floor surface area of ~1391 square feet.\n",
        "- Remodelled in 1993 \n",
        "- Lot frontage of 69 ft\n",
        "- A garage for 2 cars\n",
        "- An open porch of 25 square ft\n",
        "- 0 square ft of low quality finishes. \n",
        "- 2 full bathrooms\n",
        "- 1 fireplace\n",
        "- 487 square ft of unfinished basement.\n",
        "- 477 square ft of garage area \n",
        "- 1479 square ft of above basement living area. \n",
        "- 9536 square ft of lot area\n",
        "- 6 total rooms above the basement\n",
        "- 386 square ft of finished basement\n",
        "- Built in 1972\n",
        "- Bought in June 2008\n",
        "- Lacks all high-end and luxury assets such as a 3 season porch or pool. \n",
        "\n",
        "\n",
        "We propose the following steps\n",
        "- **[ENG]** - Try two feature engineering steps. Have `LotFrontage` be imputed to (1) 0 or (2) median (3) or mean. The choice of median vs. mean depends on the skew of the distribution, but it doesn't hurt to empirically verify. \n",
        "- **[ENG]** - Investigate `GarageYrBlt` for a good imputation value. This is either: (1) the same as the year the house itself was built or (2) not available because no garage was built. In the latter case, more sophisticated feature engineering is required. \n",
        "- **[TASK]** - Investigate all high-end and luxury features more closely.\n",
        "- **[ENG]** - Try computing the number of years in-between `YearBuilt` and `YearSold` as a feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8MwuMi6mqon"
      },
      "source": [
        "X_train[object_cols].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u4fl5B-ueCz"
      },
      "source": [
        "We make the following observations: \n",
        "- `MSZoning` - Most housing is located in \"Residential Low Density\" zones\n",
        "- `Street` - The road access to the property is predominantly paved. The classes are highly skewed.\n",
        "- `Alley` - Many values are missing. Non-missing values appear to be balanced between paved and gravel roads.\n",
        "- `LandContour` - Most housing is on level land. Unusual land shapes like banked, hillside, and depression are uncommon.\n",
        "- `Utilities` - Nearly all housing has all public utilities. This feature might be useless for determining `SalePrice`, because there is only one record that lacks all public utilities.\n",
        "- `LotConfig` - Most housing has \"Inside lot\" configuration.\n",
        "- `LandSlope` - Most housing has gentle sloping. Any other form of sloping is uncommon. It's plausible that housing with severe sloping has low `SalePrice` because this quality is undesirable.\n",
        "- `Condition1` - Most housing has Normal conditions. They are not near adverse features like noisy railroads or positive features like parks.\n",
        "- `Condition2` - Almost all housing has Normal conditions for the second condition. One way to explain the higher proportion for Normal conditions compared to `Condition1` is if `Normal` for `Condition2` means there is no second special condition near the house. Hence, almost all housing is near at most 1 special condition.\n",
        "- `BldgType` - Housing is predominantly single-family detached. \n",
        "- `HouseStyle` - About half of all housing has 1 story. The other half have additional stories.\n",
        "- `RoofStyle` - Most housing has \"gable\" roof which is is essentially the \"typical\" triangular shape of a roof.\n",
        "- `RoofMatl` - Almost all housing has \"Standard (Composite) single as the roofing material. \n",
        "- `Qual`, `Cond` - Most quality or cond features have some value denoting \"average\" which seems unsurprising. This indicates that exceptionally good or bad housing is not the majority.\n",
        "- `Heating` - Most heating uses gas. \n",
        "- `CentralAir` - Most housing uses centralized AC. This makes sense since this is likely to be the \"default\" AC system, with decentralized AC being a niche choice to fit some particular lifestyle, such as an environmentally-friendly one.\n",
        "- `Electrical` - Most housing uses \"Standard Circuit Breakers & Romex\" \n",
        "- `Functional` - Most housing has typical functionality (no damages). \n",
        "- `PavedDrive` - Most housing a paved driveway\n",
        "- `Fence` - If a house does have a fence, then it typically has minimum privacy. \n",
        "- `MiscFeature` - Most miscellaneous features, if they exist, are a simple shed.\n",
        "- `SaleType` - Most are normal warranty deeds.\n",
        "- `SaleCondition` - Most are normal sales.\n",
        "\n",
        "Many of these categorical features might fit better in a decision tree. The impact of these variables often boils down to decision-making similar to the following: \"A house with a less than typical value for `Functional` will have a severely low price due to damages driving down the property value.\" \n",
        "\n",
        "This kind of information can be encoded in linear regression using one-hot encoding, but it may overfit due to the presence of many features. One-hot encoding will also introduce many multicollinear features, as well, which is undesirable in linear regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to5HJNaNinWU"
      },
      "source": [
        "# Tracking down which columns were incorrectly typed when they were loaded\n",
        "wrong_num_cols = ['MSSubClass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysTNflnbF4yz"
      },
      "source": [
        "##### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiO1eRhQmssz"
      },
      "source": [
        "nc = null_count[null_count > 0]\n",
        "nc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkCK4qZ2nFZg"
      },
      "source": [
        "nc_prop = nc / X_train.shape[0]\n",
        "nc_prop.sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZLaOuJroNd_"
      },
      "source": [
        "We make the following observations:\n",
        "1. `PoolQC`, `MiscFeature`, `Alley`, `Fence` all have an excessive number of missing values. \n",
        "2. `FireplaceQu` nearly has half of its values missing. However, it may be reasonable to impute the 46% missing values, based on the 54% of data without missing values.\n",
        "3. The remaining features have almost negligible numbers of missing features. \n",
        "4. Based on the overview of the feature documentation we performed earlier, the data collector deliberately put in a NA value for some of the columns. Rather than trying to \"remove\" the NA values by imputing them or dropping columns, we may want to consider the NAs as their own separate category.  \n",
        "\n",
        "- **[TASK]**: Investigate `PoolQC`, `MiscFeature`, `Alley`, and `Fence` to determine why they tend to have missing values. Determine a viable imputation value or otherwise decide to discard these features.\n",
        "- **[TASK]**: For `FireplaceQu` and all of the other features determine a viable imputation value. Preserve these features if possible, since they might have useful information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tjLezaGGFNx"
      },
      "source": [
        "# Investigate the nature of the missing values in each univariate feature. It might be worth target encoding NA values in ordinal variables.\n",
        "with_missing = nc.index.to_list()\n",
        "\n",
        "# Print out the numerical columns that have missing values\n",
        "set(numerical_cols).intersection(with_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1nLimpcEHNo"
      },
      "source": [
        "# Print out the object columns that have missing vlaues\n",
        "set(object_cols).intersection(with_missing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oBZnWcjF7N0"
      },
      "source": [
        "###### GarageYrBlt\n",
        "\n",
        "From the above analysis, we see that `GarageYrBlt` is missing. This is problematic. The data description describes this feature as the year in which a garage is built. This implies that `GarageYrBlt` is missing if and only if `GarageArea` is 0. The code cell blelow confirms that this is the case.\n",
        "\n",
        "To handle missing values, we could try imputing `GarageYrBlt`, but this does not make any business sense. Essentially, imputation will enforce the assumption that every house must a garage, which is factually incorrect. \n",
        "\n",
        "Fortunately, our later analysis on the correlation between numerical variables shows that `GarageYrBlt` is highly collinear with `YearBuilt`. Moreover, since `GarageYrBlt` being null and `GarageArea == 0` are equivalent facts, we can just drop `GarageYrBlt` to handle the missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M1JjH_UEwFg"
      },
      "source": [
        "_null_mask = combined_train['GarageYrBlt'].isnull()\n",
        "_zero_mask = combined_train['GarageArea'] == 0\n",
        "\n",
        "_with_null = combined_train[_null_mask]\n",
        "_with_zero = combined_train[_zero_mask]\n",
        "_with_null_and_zero = _with_null[_with_null['GarageArea'] == 0]\n",
        "\n",
        "print(\"Total number of records with null GarageYrBlt or no GarageArea: %d\" % (len(_with_null_and_zero)))\n",
        "print(\"Total number of records with null GarageYrBlt: %d\" % (len(_with_null)))\n",
        "print(\"Total number of records with no GarageArea: %d\" % (len(_with_zero)))\n",
        "\n",
        "combined_train[_null_mask][['GarageYrBlt', 'GarageArea']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xMEs9u4H5W5"
      },
      "source": [
        "###### Lot Frontage\n",
        "\n",
        "We also see that `LotFrontage` is missing. The data description describes this as \"linear feet of street connected to property\". Ideally, NaN should indicate that there is 0 linear feet of street connected to property.  \n",
        "\n",
        "The code cell below suggests that this might be the case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH0gr9sAHNqU"
      },
      "source": [
        "print(len(combined_train[combined_train['LotFrontage'] == 0]))\n",
        "print(len(combined_train[combined_train['LotFrontage'].isnull()]))\n",
        "print(len(combined_train[combined_train['LotFrontage'] > 0]))\n",
        "print(len(combined_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWPpZZ9-YxyZ"
      },
      "source": [
        "We compute summary statistics for houses that have a `LotFrontage` value vs. houses that have a missing `LotFrontage` value to characterize houses falling into the latter category. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNrWNLVxXqu9"
      },
      "source": [
        "combined_train[combined_train['LotFrontage'].isnull()][numerical_cols + [target_col]].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Y2oaJOYll5"
      },
      "source": [
        "combined_train[~(combined_train['LotFrontage'].isnull())][numerical_cols + [target_col]].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK5FZMy1ZITm"
      },
      "source": [
        "From the above summary statistics for the numerical columns, we see that:\n",
        "- `WoodDeckSF` - Seems to be slightly higher for houses lacking `LotFrontage`\n",
        "- `PoolArea` - Appears to be slightly higher, but there are very data points that have a non-zero `PoolArea`, so the differences might be meaningless. \n",
        "- `LotArea` - Seems to be slighty higher for properties without `LotFrontage` \n",
        "- `BsmtFinSF2` - Appears to be somewhat higher for properties without `LotFrontage`\n",
        "- `BsmtFinSF1` - Appears to be slightly lower for properties without `LotFrontage`\n",
        "- `YearBuilt` - Properties without `LotFrontage` are built a few years later \n",
        "- `Fireplaces` - Slightly higher for properties without `LotFrontage` \n",
        "- `MiscVal` - Significantly higher for properties without `LotFrontage`. However, note that few properties have a miscellaneous asset, so this feature value might be distorted by outliers. \n",
        "- `LowQualFinSF` - Somewhat lower for properties without `LotFrontage` \n",
        "\n",
        "There do not seem to be particularly notable differences between the two in the numeric columns. Although `MiscVal` is higher, this is may have been distorted due to outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OHGixHYXzpi"
      },
      "source": [
        "_no_frontage_object_cols = combined_train[combined_train['LotFrontage'].isnull()][object_cols]\n",
        "_no_frontage_object_cols.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmgMYcpNbK3K"
      },
      "source": [
        "_yes_frontage_object_cols = combined_train[~(combined_train['LotFrontage'].isnull())][object_cols]\n",
        "_yes_frontage_object_cols.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJjRluKdbRH7"
      },
      "source": [
        "We observe the following differences:\n",
        "\n",
        "- `LotShape` - The houses without a `LotFrontage` are dominated by slightly irregular lots, whereas those with `LotFrontage` are more likely to have regular lots. \n",
        "- `Foundation` - Houses without a `LotFrontage` tend to have a cinderblock foundation, whereas for houses with a `LotFrontage`, a concrete foundation has the highest proportion of members (though it does not make up more than half of the observations).\n",
        "- `BsmtQual` - More than half of the basements have good quality for houses without a `LotFrontage`. From the analysis of numerical columns, houses without a `LotFrontage` tend to have less unfinished garage area. \n",
        "- `BsmtFinType` - Houses with no `LotFrontage` tend to have \"good living quarters\" as the finish type with the highest members. House with `LotFrontage` have no basement finish as the dominant finish type.\n",
        "- `FireplaceQu` - Houses with no `LotFrontage` tend to have average quality fireplaces, in contrast to good quality. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXtlS-v4fqgJ"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "sns.countplot(ax=ax[0], data=_no_frontage_object_cols, x=\"LotShape\")\n",
        "sns.countplot(ax=ax[1], data=_yes_frontage_object_cols, x=\"LotShape\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92OyQcciiRuo"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "sns.countplot(ax=ax[0], data=_no_frontage_object_cols, x=\"Foundation\")\n",
        "sns.countplot(ax=ax[1], data=_yes_frontage_object_cols, x=\"Foundation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKs1fPnyjb74"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "sns.countplot(ax=ax[0], data=_no_frontage_object_cols, x=\"GarageType\")\n",
        "sns.countplot(ax=ax[1], data=_yes_frontage_object_cols, x=\"GarageType\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxiZ-Ej4j493"
      },
      "source": [
        "The difference between houses with and without `LotFrontage` is unclear. We cannot find a systematic reason why `LotFrontage` is missing. \n",
        "\n",
        "Hence, it is not obvious what the correct imputation value for `LotFrontage` is. An NA value could denote 0 lot frontage or that the non-zero lot frontage is not recorded. We will defer to experiments that empirically check which imputation method offers the best performance. \n",
        "\n",
        "- **[ENG]** - Try imputing `LotFrontage` with the mean, median, constant 0, or dropping the feature entirely. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB04BGnZkv6e"
      },
      "source": [
        "###### MasVnrArea\n",
        "\n",
        "In the code cell below, we see for `MasVnrArea` that NA does not denote an area of 0, since there are records with `MasVnrArea == 0`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTEelOzhk8rW"
      },
      "source": [
        "_equals_zero = combined_train[combined_train['MasVnrArea'] == 0]\n",
        "_gt_zero = combined_train[combined_train['MasVnrArea'] > 0]\n",
        "_na = combined_train[combined_train['MasVnrArea'].isnull()]\n",
        "\n",
        "print(\"MasVnrArea == 0: %d\"           % len(_equals_zero))\n",
        "print(\"MasVnrArea not available: %d\"  % len(_na))\n",
        "print(\"MasVnrArea > 0: %d\"            % len(_gt_zero))\n",
        "print(\"Total number of records: %d \"  % len(combined_train))\n",
        "print(\"%d + %d + %d = %d\"             % (len(_equals_zero), \n",
        "                                         len(_na), \n",
        "                                         len(_gt_zero), \n",
        "                                         len(_equals_zero) + len(_na) + len(_gt_zero)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrM6LHkrnNHC"
      },
      "source": [
        "We check the corresponding `MasVnrType` to better understand why `MasVnrArea` is missing. We see that either feature is missing if and only if the other is missing as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVgXQlqFmgGw"
      },
      "source": [
        "print(\"The corresponding veneer type of records without masonry veneer area\")\n",
        "_na[['MasVnrArea', 'MasVnrType']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_UomXFym1Bv"
      },
      "source": [
        "print(\"The number of records without a masonry veneer type: %d\" %\n",
        "      len(combined_train[combined_train['MasVnrType'].isnull()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjBF-7K0nYlU"
      },
      "source": [
        "Thus, the `MasVnrType` appears to be missing. This is interesting, because there are records where `MasVnrArea == 0`. The next question is what is the `MasVnrType` when `MasVnrArea == 0`? We show a sample of these records below. This demonstrates that, roughly speaking, `MasVnrType == None` if and only if `MasVnrArea == 0`. \n",
        "\n",
        "There is however, one outlier where the `MasVnrType` is `Stone` but the `MasVnrArea` is recorded as 0. This record appears to be incorrect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukOwl9munBSI"
      },
      "source": [
        "_equals_zero[['MasVnrArea', 'MasVnrType']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppp3wneanuov"
      },
      "source": [
        "print(np.unique(_equals_zero['MasVnrArea']))\n",
        "print(np.unique(_equals_zero['MasVnrType']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ8xYf30n4HL"
      },
      "source": [
        "_equals_zero[_equals_zero['MasVnrType'] == 'Stone']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di-sZjyspDeT"
      },
      "source": [
        "Compared to the average house illustrated by the summary statistics, the outlier house with id 1242 has\n",
        "1. `BsmtUnfSF` - A high value of 1689\n",
        "2. `1stFlrSF` - A high value of 1689\n",
        "3. `SalePrice` - Significantly higher than the mean or median. \n",
        "4. `SaleCondition` - This house is being sold as `Partial`\n",
        "\n",
        "According to the data documentation, we see that \"Partial\" means that \"Home was not completed when last assessed (associated with New Homes)\". This would explain the discrepancy, as the house might have been planned to have stone masonry, but when the house was assessed, the construction of the masonry was not started. \n",
        "\n",
        "For simplicity, we can drop this outlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpxMTOrrkIX"
      },
      "source": [
        "Finally, we try to find any systematic reason for why `MasVnrArea` is missing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObdFk5V3sTD5"
      },
      "source": [
        "_na.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wlz9vuSsiU7"
      },
      "source": [
        "_na[object_cols].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaiz3CwLtEZN"
      },
      "source": [
        "If you compare the above summary statistics above with the summary statistics for the overall training set, there does not appear to be any noteworthy difference. \n",
        "\n",
        "We attempt two strategies for feature engineering: (1) impute `MasVnrArea` using the median/mean and `MasVnrType` with the most frequent observation, (2) first impute `MasVnrType` with the most frequent observation, then impute `MasVnrArea` with the average `MasVnrArea` for the most frequent `MasVnrType`.\n",
        "\n",
        "The second approach might offer better accuracy, but we will empirically verify this.\n",
        "\n",
        "- **[ENG]** - Try the two feature engineering strategies above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV2zBYKmYeLc"
      },
      "source": [
        "###### Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds4ahaWqxT_Z"
      },
      "source": [
        "According to the data documentation, the following categorical features use \"NA\" to denote a known absence of some feature like alley-way access, rather than unknown information. We can replace \"NA\" in the `DataFrame` with `\"Absent\"` so that this NA category is accounted for by computations, such as  Cramer's V, which used in the categorical to categorical correlation matrix later in this notebook.\n",
        "\n",
        "- `Alley` - NA means no alley access\n",
        "- `BsmtCond` - NA means no basement\n",
        "- `BsmtExposure` - NA means no basement\n",
        "- `BsmtFinType1` - NA means no basement\n",
        "- `BsmtFinType2` - NA means no basement\n",
        "- `BsmtQual` - NA means no basement\n",
        "- `Fence` - NA means no fence\n",
        "- `FireplaceQu` - NA means no fireplace\n",
        "- `GarageCond` - NA means no garage\n",
        "- `GarageFinish` - NA means no garage\n",
        "- `GarageQual` - NA means no garage\n",
        "- `GarageType` - NA means no garage\n",
        "- `MiscFeature` - NA means no misc feature\n",
        "- `PoolQC` - NA means no pool\n",
        "\n",
        "The following features use \"NA\" to denote unknown information, and thus, these values require feature engineering. \n",
        "- `Electrical` - Seems acceptable to impute using most frequent. \n",
        "- `MasVnrType` - Also discussed earlier in the `MasVnrArea` section\n",
        "\n",
        "\n",
        "- **[ENG]** - Replace \"NA\" with its own category for the features in the first list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LNnXpyHzGwT"
      },
      "source": [
        "NA_category = 'Absent'\n",
        "cols_with_NA_category = ['Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
        "                         'BsmtFinType2', 'BsmtQual', 'Fence', 'FireplaceQu', \n",
        "                         'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', \n",
        "                         'MiscFeature', 'PoolQC']\n",
        "\n",
        "\n",
        "combined_train_with_absent = combined_train.copy()\n",
        "combined_train_with_absent[cols_with_NA_category] = combined_train[cols_with_NA_category].fillna(NA_category)\n",
        "combined_train_with_absent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xla1Z-mizFUq"
      },
      "source": [
        "###### Electrical\n",
        "\n",
        "According to the data documentation, there should be no house that has the category `NA`. For simplicity, we will try to impute using two strategies: (1) using most frequent, (2) using \"Absent\"\n",
        "\n",
        "- **[ENG]** - Impute the category for `Electrical`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL5deTH_F9Z9"
      },
      "source": [
        "##### Distributions for Numeric Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO3dMRXx6k8F"
      },
      "source": [
        "# Plots all features in `cols` on a `nrows x ncols` grid. Each feature is\n",
        "# graphed in the specified `plot` type.\n",
        "def plot_grid(plot, cols, nrows=3, ncols=3, figlen=5):\n",
        "  fig, ax = plt.subplots(nrows=nrows, ncols=ncols, \n",
        "                         figsize=(figlen * ncols, figlen * nrows))\n",
        "  \n",
        "  for k, feat in enumerate(cols):\n",
        "    i = k // ncols\n",
        "    j = k % ncols\n",
        "\n",
        "    plot(ax[i][j], feat)\n",
        "\n",
        "  for k in range(nrows * ncols - 1, len(cols) - 1, -1):\n",
        "    i = k // ncols\n",
        "    j = k % ncols\n",
        "    plt.delaxes(ax[i][j])\n",
        "\n",
        "  plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaXFXd-QE-gj"
      },
      "source": [
        "# Perform adjustments to the histogram for \"discrete\" variables, that is,\n",
        "# numerical variables consisting of a handful of discrete integers as values.\n",
        "def discrete_plot(kwargs, ax, uniq_vals = 5):\n",
        "  kwargs['bins'] = [i for i in range(0, uniq_vals + 2, 1)]\n",
        "  ax.set_xticks([i for i in range(0, uniq_vals + 2, 2)])\n",
        "  kwargs['stat'] = 'count'\n",
        "  kwargs['kde'] = False\n",
        "\n",
        "# Perform adjustments to the histogram for \"year\" variables like `YearBuilt`\n",
        "def timeline_plot(kwargs, ax):\n",
        "  kwargs['bins'] = np.arange(1870, 2030, 5) \n",
        "\n",
        "# Perform adjustments to the histogram for \"continuous\" numerical variables.\n",
        "def continuous_plot(kwargs, ax, min, max, step, label_factor=2):\n",
        "  kwargs['bins'] = np.arange(min, max, step)\n",
        "  ax.set_xticks(np.arange(min, max + step, label_factor * step))\n",
        "\n",
        "discrete_features = ['BedroomAbvGr', 'BsmtHalfBath', 'BsmtFullBath', \n",
        "                     'GarageCars', 'KitchenAbvGr', 'FullBath', 'HalfBath',\n",
        "                     'TotRmsAbvGrd', 'KitchenAbvGr', 'Fireplaces',\n",
        "                     'OverallQual', 'OverallCond', 'MoSold']\n",
        "\n",
        "def pre_adjustment(ax, feat):\n",
        "  kwargs = { 'stat' : 'density', 'kde' : True }\n",
        "\n",
        "  if feat in discrete_features:\n",
        "    discrete_plot(kwargs, ax, len(np.unique(X_train[feat])))\n",
        "\n",
        "  # feature-specific adjustments to the graph to make it more readable.\n",
        "\n",
        "  if feat == 'BsmtFinSF1':\n",
        "    continuous_plot(kwargs, ax, 0, 2400, 100, 4)\n",
        "\n",
        "  if feat == 'BsmtFinSF2':\n",
        "    continuous_plot(kwargs, ax, 0, 1200, 100)\n",
        "\n",
        "  if feat == 'YearRemodAdd':\n",
        "    continuous_plot(kwargs, ax, 1950, 2030, 5)\n",
        "\n",
        "  if feat == 'TotalBsmtSF':\n",
        "    continuous_plot(kwargs, ax, 0, 3300, 100, 4)\n",
        "\n",
        "  if feat == 'ScreenPorch':\n",
        "    ax.set_ylim([0, 0.023])\n",
        "\n",
        "  if feat == 'YrSold':\n",
        "    continuous_plot(kwargs, ax, 2006, 2012, 1)\n",
        "    kwargs['kde'] = False\n",
        "\n",
        "  if feat == 'BsmtUnfSF':\n",
        "    continuous_plot(kwargs, ax, 0, 2000, 100)\n",
        "\n",
        "  if feat == 'LotFrontage':\n",
        "    continuous_plot(kwargs, ax, 0, 320, 20)\n",
        "\n",
        "  if feat == 'OpenPorchSF':\n",
        "    continuous_plot(kwargs, ax, 0, 550, 50)\n",
        "\n",
        "  if feat == 'PoolArea':\n",
        "    continuous_plot(kwargs, ax, 0, 800, 100)\n",
        "    kwargs['stat'] = 'count'\n",
        "    kwargs['kde'] = False\n",
        "\n",
        "  if feat == 'YearBuilt':\n",
        "    timeline_plot(kwargs, ax)\n",
        "\n",
        "  if feat == 'WoodDeckSF':\n",
        "    continuous_plot(kwargs, ax, 0, 750, 50)\n",
        "\n",
        "  if feat == 'LowQualFinSF':\n",
        "    ax.set_ylim([0, 0.020])\n",
        "\n",
        "  if feat == 'LotArea':\n",
        "    continuous_plot(kwargs, ax, 1000, 216000, 5000, label_factor = 8)\n",
        "\n",
        "  if feat == 'MiscVal':\n",
        "    continuous_plot(kwargs, ax, 0, 17000, 1000)\n",
        "    ax.set_ylim([0, 0.0010])\n",
        "\n",
        "  if feat == 'GarageYrBlt':\n",
        "    timeline_plot(kwargs, ax)\n",
        "\n",
        "  if feat == '3SsnPorch':\n",
        "    ax.set_ylim([0, 0.03])\n",
        "\n",
        "  if feat == '1stFlrSF':\n",
        "    continuous_plot(kwargs, ax, 0, 3600, 200)\n",
        "\n",
        "  if feat == '2ndFlrSF':\n",
        "    continuous_plot(kwargs, ax, 0, 3600, 200)\n",
        "\n",
        "  if feat == 'GarageArea':\n",
        "    continuous_plot(kwargs, ax, 0, 1400, 50, label_factor = 4)\n",
        "\n",
        "  return kwargs\n",
        "\n",
        "def plot_dist(ax, feat):\n",
        "  kwargs = pre_adjustment(ax, feat)\n",
        "  subax = sns.histplot(ax=ax, data=X_train, x=feat, **kwargs)\n",
        "  return subax\n",
        "\n",
        "plot_grid(plot_dist, list(set(numerical_cols) - set(wrong_num_cols)), nrows=6, ncols=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvv_bwrdFtPo"
      },
      "source": [
        "There are a few interesting distributions, that do not appear to be unimodal.\n",
        "- `BsmtFinSF1` - Bimodal with peaks at `[0,100]` and `[600, 700]`\n",
        "- `BsmtFinSF2` - Unimodal with peak at `[0, 100]`. This notable because the behaviour is different from `BsmtFinSF1`\n",
        "- `YearRemodAdd` - This is trimodal. There are three peaks: `[1950, 1955]`, somewhere in `[1965, 1980]`, and `[2005, 2010]` \n",
        "- `BsmtUnfSF` - The distribution may be bimodal. The first peak is around `[0, 100]` and the second peak is around `[600, 700]`. Hence, houses appear to come in populations, one where there is unfinished basement surface area. One where there is unfinished surface area, where the median or mean is in `[600, 700]`. It is possible that the first peak is so large, because houses without a basement would have technically have 0 unfinished surface area.\n",
        "- `WoodDeckSF` - Bimodal with two peaks: `[0, 50]` and `[100, 150]`\n",
        "- `EnclosedPorch` - Bimodal with peaks: `[0, 50]` and `[100, 150]`\n",
        "- `2ndFlrSF` - Bimodal for with two peaks `[0, 200]` and `[600, 800]`\n",
        "- `TotalBsmtSF` - Bimodal. One population like has no basement, `[0, 100]` and one population has `[800, 900]` sq ft of basement.\n",
        "- `GarageArea` - Possibly trimodal with two peaks `[0, 100]`, `[250, 300]`, and `[500, 550]`. \n",
        "- `MoSold` - The data appears to be bimodal. Housing sales peak at Month 6 (June) and Month 10 (October).\n",
        "\n",
        "The year in which housing is being built show trimodal distributions. Moreover, they show similar distribution shapes.\n",
        "- `YearBuilt` - This distribution appears to be trimodal. There were booms in housing construction centered around the years `[1920, 1925]`, `[1965, 1970]`, and `[2005, 2010]`. For the third boom, this might be potentially related to the 2008 U.S. housing bubble.\n",
        "- `GarageYrBlt` - Has roughly the same distribution as `YearBuilt`. One explanation is that most houses are built with their garage. It is highly possible that these two features are multicollinear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FqQIvxzHo4O"
      },
      "source": [
        "##### Barplots for Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMhmWJrzEkal"
      },
      "source": [
        "def plot_catcount(data=combined_train_with_absent):\n",
        "  def _plot(ax, feat):\n",
        "    subax = sns.countplot(ax=ax, data=data, x=feat)\n",
        "    subax.set_xticklabels(subax.get_xticklabels(), rotation=45)\n",
        "    return subax\n",
        "  return _plot\n",
        "\n",
        "plot_grid(plot_catcount(), object_cols + wrong_num_cols, nrows=7, ncols=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD5ON7wYCUnL"
      },
      "source": [
        "The barplots yields the following new information: \n",
        "- `MSZoning` - Values from most frequent to least frequent are: \"Residential Low Density\", \"Residential Medium Density\", \"Floating Village Residential\", \"Commercial\", and \"Residential High Density\". Interestingly, the commercial zoning has value \"C (all)\" instead of \"C\", the latter of which is used in the data documentation. It is unclear if \"C (all)\" has any special meaning over \"C\",\n",
        "- `LotShape` - Most houses have regular lot shapes, about half as much have \"Slightly irregular\" shapes. A few houses have either moderately irregular or irregular. \n",
        "- `LotConfig` - ~750 houses have an \"inside\" lot config, a ~200 have \"corner.\" The other values make up the rest.\n",
        "- `BldgType` - Values from most frequent to least frequent are \"Single-family detached\", \"Townhouse End Unit\", \"Duplex\", \"Townhouse Inside unit\" (?), \"Two-family conversion.\" The data documentation is incorrect, since `\"Twnhs\"` is the actual value representing \"Townhouse Inside Unit\". \n",
        "- `HouseStyle` - ~500 houses are 1story, ~350 are 2story. The other categories make up the rest. \n",
        "- `RooflStyle` - ~800 houses have gable roofs, ~200 have hip roofs.\n",
        "- `Exterior1st` - From most to least frequent: \"Vinyl Siding\", \"Hard Board\", \"Wood Siding\", \"Metal Siding\", and then the rest. \n",
        "- `Exterior2nd` - From most to least frequent: \"Vinyl Siding\", \"Wood Siding\", \"Wood Shingles\", \"Hard Board\", \"Plywood\" and then the rest. \n",
        "- `MasVnrType` - ~600 houses have no masonry veneer, whereas ~300 have brickface, ~100 have stone. The rest have \"Brick Common\" \n",
        "- `ExterQual` - ~650 houses have average exterior quality, whereas ~350 have good exterior quality. The rest have either excellent or fair. \n",
        "- `Foundation` - ~450 houses have poured concrete foundations, ~450 have cinder block. \n",
        "- `BsmtQual` - ~450 houses have average basement quality, slighty less houses have good quality. The rest are fair or excellent.\n",
        "- `BsmtFnType` - ~300 are unfinished, slightly less than that have \"Good Living Quarters\" \n",
        "- `HeatingQC` - ~500 are in excellent quality, ~300 average, ~200 good. \n",
        "- `KitchenQual` - ~500 are average, ~400 are good.\n",
        "- `GarageType` - ~600 are attached, ~300 are detached. \n",
        "- `GarageFinish` - The finish is split between ~400 unfinished, ~300 rough finished, ~250 finished. \n",
        "- `Fence` - ~100 provide minimum privacy, ~45 have good privacy, ~40 have good wood, and then the rest.\n",
        "- `MSSubClass` - Most of the housing sales occur for code 20 (1-story 1946 and newer) and 60 (2-story 1946 and newer). One way of explaining this is that newer houses are more desirable than older ones, particular those built earlier than 1946, which is more than 60 years ago from the earliest sell date in the data, 2006. \n",
        "- There are a few graphs with skewed classes, which might distort the linear regression model, such as: `Alley`, `PoolQc`, `MiscFeature`. The overwhelming majority of observations have the value `\"Absent\"` and our model might not be able to estimate the benefit when these assets are not absent, since there are too few data points to learn from. \n",
        "- The rest of the quality variables follow a similar pattern to ones already mentioned. There are no unusual bar graphs. \n",
        "\n",
        "- **[TASK]** - Create a boxplot for `SalePrice` against `Street`, `Neighborhood`, `Condition1`, `Exterior1st`, `Exterior2nd`\n",
        "- **[TASK]** - Consider dropping `Alley`, `PoolQc`, `MiscFeature` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQeG-X1aD-i0"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHjt854I4LT-"
      },
      "source": [
        "#### SalePrice vs. Numeric Predictors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkxhinARJp8m"
      },
      "source": [
        "# Scatterplots with each numerical feature \n",
        "def plot_regression(ax, feat):\n",
        "  subax = sns.regplot(ax=ax, data=combined_train, x=feat, y='SalePrice')\n",
        "  return subax\n",
        "\n",
        "plot_grid(plot_regression, list(set(numerical_cols) - set(wrong_num_cols)), nrows=6, ncols=6, figlen=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8r3u-OX7yRy"
      },
      "source": [
        "We make the following observations. \n",
        "\n",
        "First, the following features have scatterplats that indicate that homoscedasticity is violated, since the points are spread out so they have a \"fanning\" pattern\n",
        "\n",
        "- `YearRemodAdd` \n",
        "- `GarageYrBlt`\n",
        "- `1stFlrSF`\n",
        "- `MasVnrArea`\n",
        "- `GarageArea` \n",
        "- `LotFrontage`\n",
        "- `YearBuilt`? \n",
        "- `OpenPorchSF`\n",
        "- `EnclosedPorch`\n",
        "- `TotalBsmtSF`\n",
        "- `GrLivArea` \n",
        "- `WoodDeckSF`? \n",
        "\n",
        "The following features might satisfy homoscedasticity\n",
        "\n",
        "- `BsmtUnfSF`\n",
        "- `BsmtFinSF1`\n",
        "- `BsmtFinSF2`?\n",
        "- `2ndFlrSF`? \n",
        "\n",
        "All other features have an unclear with homoscedasticity. For example, `LowQualFinSF` might violate homoscedasticity, but this could be a visual distortion due to a small number of data points. In the \"Model Assumptions\" section, we make additional plots to confirm any violations of homoscedasticity.\n",
        "\n",
        "Additional, we also note that the following features have a down-sloping regression line.\n",
        "\n",
        "- `MiscVal` - This result might be distorted due to the fact there are few data points. \n",
        "- `BsmtUnfSF`\n",
        "- `EnclosedPorch` - This result might be distorted due to the high number of data points without any enclosed porch.\n",
        "- `KitchenAbvGr` - Might be distorted since the x-variable has discrete values.\n",
        "- `BsmtHalfBath` - Might be distroted since the x-variable has discrete values.\n",
        "- `YrSold`? - The slope appears to be close to 0, so the relationship is very mild.\n",
        "- `OverallCond` - Might be distorted by the number of \"average\" condition scatterpoints.\n",
        "- `LowQualFinSF` - Might be distorted by the small number of points.  \n",
        "\n",
        "Hence, it seems as though many of the negative relationships are being distorted due to quirks with the data, e.g, discrete feature values or a high number of points with feature value 0. All other features appear to have positive relationships, as indicated by an upward-sloping regression line. \n",
        "\n",
        "The following features' scatterplots appear to be distorted by a high number of points with value 0. \n",
        "\n",
        "- `TotalBsmtSF`\n",
        "- `WoodDeckSF`\n",
        "- `BsmtFinSF1`\n",
        "- `BsmtFinSF2`\n",
        "- `LowQualFinSF`\n",
        "- `3SsnPorch`\n",
        "- `PoolArea`\n",
        "- `2ndFlrSF`\n",
        "\n",
        "We should check the regression after the points with value 0 are removed to better understand the relationship of the feature with `SalePrice`. This is justified because often when a data point has predictor value 0, it indicates that it doesn't exist, e.g, `WoodDeckSF = 0` indicates no wooden deck. These points will have high variability in `SalePrice`, because other features will take on wildly different values. Hence, these points are somewhat exceptional. \n",
        "\n",
        "We can therefore treat each listed feature above as having two populations: one with zero predictor value and with non-zero predictor value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQPfsxXM_Hie"
      },
      "source": [
        "# Adjusting some of the plots above, by removing data points where feature value is 0. \n",
        "\n",
        "def plot_scatter(ax, feat):\n",
        "  nonzero = combined_train[combined_train[feat] > 0]\n",
        "  subax = sns.regplot(ax=ax, data=nonzero, x=feat, y='SalePrice')\n",
        "  return subax\n",
        "\n",
        "with_zeros = ['TotalBsmtSF', 'WoodDeckSF', 'BsmtFinSF1', 'BsmtFinSF2', 'LowQualFinSF', '3SsnPorch', 'PoolArea', '2ndFlrSF']\n",
        "plot_grid(plot_scatter, with_zeros, nrows=2, ncols=4, figlen=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C54XagN44T_e"
      },
      "source": [
        "#### SalePrice vs. Categorical Predictors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHbQaSlt1MQb"
      },
      "source": [
        "# Boxplots of the categorical features with `SalePrice` on the y-axis.\n",
        "def plot_boxplot(ax, feat):\n",
        "  # Plotting the actual boxes in order of lowest to highest median\n",
        "  order = combined_train_with_absent.groupby(by=[feat])[\"SalePrice\"].median().sort_values().index\n",
        "  subax = sns.boxplot(ax=ax, data=combined_train_with_absent, x=feat, y='SalePrice', order=order)\n",
        "\n",
        "  # Rotating the x-labels to be readable\n",
        "  subax.set_xticklabels(subax.get_xticklabels(), rotation=45)\n",
        "\n",
        "  for pos, label in enumerate(ax.get_xticklabels()):  \n",
        "      series = combined_train_with_absent[combined_train_with_absent[feat].astype('string') == label.get_text()]\n",
        "      median = series['SalePrice'].median()\n",
        "      mean = series['SalePrice'].mean()\n",
        "      n = len(series)\n",
        "\n",
        "      ax.text(pos,\n",
        "              median + 0.04,\n",
        "              \"n: {n:d}\\nmu: {mean:.0f}\".format(n=n, mean=mean),\n",
        "              horizontalalignment='center',\n",
        "              size='x-small',\n",
        "              color='w',\n",
        "              weight='semibold')\n",
        "\n",
        "  return subax\n",
        "\n",
        "plot_grid(plot_boxplot, object_cols + wrong_num_cols, nrows=7, ncols=7, figlen=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yluKDhkcUi-H"
      },
      "source": [
        "Above, we have a boxplot of every categorical feature against the `SalePrice`. We have annotated each box with the number of elements with that categorical value and the mean `SalePrice` for that category. \n",
        "\n",
        "We make the following observations. \n",
        "\n",
        "- `MSZoning` - Surprisingly, \"Floating Village\" has a high average and median price. It is not clear what this residential zone is meant to represent. \"Commercial\" zones tend to have the lowest `SalePrice` . Whereas residential zones occupy the \"middle\" with low-density residential areas being the highest. Low-density residential areas being the highest might make sense, if houses in this area take up a lot of land. Then, an individual person must pay for more land, than those in low-density zones. \n",
        "- `Street` - Paved road access to property is associated with a higher median and mean `SalePrice`. This might illustrate a dichotomy between rural (gravel roads) vs. urban (paved roads) housing. \n",
        "- `LotShape` - The more irregular the lot is, the higher the median `SalePrice`. We could target encode `LotShape` by median `SalePrice`\n",
        "- `LandContour` - Level and banked land is associated with lower `SalePrice`, whereas depressed or hillside land is associated with higher `SalePrice` \n",
        "- `Utilities` - This is dominated by housing that has all utilities, so we cannot contrast different groups.\n",
        "- `LotConfig` - FR3 has too few examples to grasp its properties. Inside, Corner, and Frontage on 2 Sides are roughly similar in price. A CulDSac config is associated with a higher `SalePrice`. \n",
        "- `LandSlope` - A `Gtl` slope is priced less than moderate or severe, which are equally as expensive. Encoding by median `SalePrice` does not follow the natural ordering of `Gtl`, `Mod`, `Sev`. We might want to consider mean encoding or simply using median encoding. \n",
        "- `Neighborhood` - As expected, the boxplot visually confirms that the neighborhood can make a significant difference as to what the price is. Some neighborhoods tend to have more expensive houses than others.\n",
        "- `Condition1` - Arterials have the lowest `SalePrice`. This does not disprove our initial assumptions, because an arterial road is likely to be noisy due to high traffic flow. The noise polluation would most likely drive down `SalePrice`. Similar logic applies to a feeder street. Both `RRAn` and `RRAe` imply that a house is relatively close to a railroad, which agrees with our assumption. On the other hand, parks and positive features, indicated by `PosN` and `PosA`, drives up `SalePrice`. The `RRNn` and `RRNe` variables are difficult to interpret, since it is unclear what \"200'\" means in the data description. This likely refers to properties that are within 200 units of a railroad, but not directly beside it. This might explain why the `SalePrice` is not lower than the `Norm` condition, but it does not explain why it is higher. \n",
        "- `BldgType` - The boxplot seems to reveal the following trend: houses that offer more privacy and has fewer families living in a single unit have a higher `SalePrice`. A \"Single-family\" detached has the highest sale price. A townhouse end-unit is a higher sale price than the inside unit, because it provides more privacy according to [this](https://firsthousecoach.com/is-an-end-unit-townhouse-worth-more/) article. The Duplex home does break this trend, as a duplex could be inhabited a single family. \n",
        "- `RoofMatl` - Wooden shake and wood shingle houses are rare and are associated with housing with very high `SalePrice`. A tar and gravel roof is also rare and it seems to be associated with slightly higher `SalePrice`. Roofs typically use composite shingles. \n",
        "- `MasVnrType` - Veneer made from stone or break-face appears to be associated with expensive housing.\n",
        "- `ExterQual` - Higher quality is associated with higher `SalePrice`\n",
        "- `ExterCond` - This feature appears to be less influential compared to `ExterQual` \n",
        "- `BsmtQual` - Higher quality, higher `SalePrice`\n",
        "- `BsmtCond` - Clear relationship with `SalePrice`, but less influential than `BsmtQual`. \n",
        "\n",
        "- Generally speaking higher values for \"quality\" or \"condition\" features leads to an increase to `SalePrice` as expected. \n",
        "\n",
        "For most of the features, the categorical classes can be target encoding using the median `SalePrice`. This kind of encoding could be useful for capturing nonlinearity between a feature and the `SalePrice` \n",
        "\n",
        "- **[TASK]** - Determine what a floating village is and why it would be so expensive.\n",
        "- **[TASK]** - Add categorical predictors one at a time to increase performance without introducing problems due to many one-hot encoded features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AWRm471D-1U"
      },
      "source": [
        "### Multivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBXhFBZMTaQs"
      },
      "source": [
        "#### Numeric vs. Numeric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjocDKC4DR0m"
      },
      "source": [
        "numerical_cols_2 = list(set(numerical_cols) - set(wrong_num_cols))\n",
        "corrmat = combined_train[numerical_cols_2 + [target_col]].corr()\n",
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(corrmat, ax=ax, square=True, annot=True, vmin=-1, vmax=1)\n",
        "ax.set_title(\"Correlation matrix for all numeric features\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXM9kWfNUyVs"
      },
      "source": [
        "# Define thresholds for pairs of features that have a notable amount of correlation\n",
        "low_threshold = 0.30\n",
        "med_threshold = 0.50\n",
        "high_threshold = 0.70\n",
        "\n",
        "def get_feats_with_min_cor(corrmat, threshold):\n",
        "  _corrmat = (corrmat.abs() * 1000).round(0)\n",
        "  _within_threshold = 1000 * threshold <= _corrmat\n",
        "  _not_on_diagonal = _corrmat < 1000\n",
        "  _not_null = corrmat != np.nan\n",
        "  mask = (_within_threshold & _not_on_diagonal & _not_null).sum() > 0\n",
        "\n",
        "  feats = mask[mask].index\n",
        "  return list(feats)\n",
        "\n",
        "def get_feat_corr_hierarchy(corrmat):\n",
        "  # Determine the features with at least one entry that has a correlation above the threshold. Such features are worth further investigation.\n",
        "  all_feats = get_feats_with_min_cor(corrmat, 0)\n",
        "  low_feats = get_feats_with_min_cor(corrmat, low_threshold)    # features which has at least 1 entry, with at least 0.30 correlation\n",
        "  med_feats = get_feats_with_min_cor(corrmat, med_threshold)    # features which has at least 1 entry, with at least 0.50 correlation\n",
        "  high_feats = get_feats_with_min_cor(corrmat, high_threshold)  # features which has at least 1 entry, with at least 0.70 correlation\n",
        "  none_feats = list(set(all_feats) - set(low_feats))            # features with no entry having correlation greater than or equal to 0.30\n",
        "  return low_feats, med_feats, high_feats, none_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnhlt42N3GMR"
      },
      "source": [
        "low_feats, med_feats, high_feats, none_feats = get_feat_corr_hierarchy(corrmat)\n",
        "\n",
        "print(none_feats)\n",
        "print(low_feats)\n",
        "print(med_feats)\n",
        "print(high_feats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s08mrrR1XzvG"
      },
      "source": [
        "low_corrmat = combined_train[low_feats].corr()\n",
        "med_corrmat = combined_train[med_feats].corr()\n",
        "high_corrmat = combined_train[high_feats].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHavpX6qX7-s"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(low_corrmat, ax=ax, square=True, annot=True, vmin=-1, vmax=1)\n",
        "ax.set_title(\"Features with at least 1 low correlation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek2yuKPwpOCM"
      },
      "source": [
        "Above is the low correlation matrix. We will ignore this matrix for now, because pairs of features with correlation between $[0.10, 0.30]$ might be negligible for our linear regression model. Features with low correlation in the range $[0.30, 0.50]$ might negatively impact the linear regression model, but perhaps far less than moderately or highly correlated features.\n",
        "\n",
        "If additional performance increases are necessary, we may consider dropping features from the low correlation matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzpshqPnYAJV"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(med_corrmat, ax=ax, square=True, annot=True, vmin=-1, vmax=1)\n",
        "ax.set_title(\"Features with at least 1 medium correlation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiXzwFuZd9fc"
      },
      "source": [
        "In the medium correlation matrix, we note the following medium correlation relationships, that is, entries with correlation values in $[0.50, 0.70)$.\n",
        "\n",
        "- `YearRemodAdd`, `GarageYrBlt` - This relationship is peculiar, because there is a moderate, positive linear relationship. One hypothesis that explains this correlation is that garages are often part of a remodelling, so `YearRemodAdd` is approximately equal to `GarageYrBlt` for several entries, which leads to this high correlation. \n",
        "    \n",
        "    From the high correlation analysis, we are already discarding `GarageYrBlt`, so there is no action to be done.\n",
        "\n",
        "- `YearRemodAdd`, `YearBuilt` - As shown in the high correlation matrix analysis, `YearBuilt` and `GarageYrBlt` are highly correlated, and we saw earlier that `GarageYrBlt` and `YearRemodAdd` are correlated, so this correlation might be explained by transitivity. \n",
        "\n",
        "    Intuitively, the `YearRemodAdd` feature does not appear to be redundant with `YearBuilt`, so we will not discard anything.\n",
        "\n",
        "- `GarageYrBlt`, `GarageArea` - This relationship is **surprising**. It is difficult to explain why there appears to be a moderate correlation. One hypothesis is that as a city develops over time, there is a greater need to drive, such as going to work. Therefore, as time goes on, more houses are built or remodelled to accomodate more cars. \n",
        "\n",
        "    From the high correlation analysis, we already decided to discard `GarageYrBlt`, so no action is required.\n",
        "\n",
        "- `GarageYrBuilt`, `GarageCars` - `GarageCars` is strongly correlated with `GarageArea` so the same reasoning as above applies. \n",
        "\n",
        "    From the high correlation analysis, we already decided to discard both features, so no action is required.\n",
        "\n",
        "- `GarageYrBuilt`, `OverallQual` - We know from the high correlation analysis that `GarageYrBuilt` is highly correlated with `YrBuilt`. We also see that `YrBuilt` has a moderate correlation with `OverallQual`. Therefore, by transitivity, `GarageYrBuilt`, `OverallQual` are moderately correlated. To explain the correlation between `YrBuilt` and `Overall`, one hypothesis is that as a city develops over time, the housing constructed has better quality overall. \n",
        "\n",
        "    From the high correlation analysis, we already decided to discard `GarageYrBlt`, so no action is required.\n",
        "\n",
        "- `BsmtFullBath`, `BsmtFinSF1` \n",
        "\n",
        "    We could discard `BsmtFullBath` since it is a discrete variable, as long as it provides a real performance improvement\n",
        "\n",
        "- `TotRmsAbvGrd`, X \n",
        "  \n",
        "    Skipping this row with `TotRmsAbvGrd`, since we have already decided to discard this feature in the high correlation analysis.\n",
        "\n",
        "- `BsmtUnfSF`, `BsmtFinSF1`- This is a negative relationship, which makes intuitive sense since the features refer to opposite quantities.\n",
        "\n",
        "    We could try discarding one feature or the other.\n",
        "\n",
        "- `1stFlrSF`, `GrLivArea` \n",
        "\n",
        "    We could try discarding one feature or the other.\n",
        "\n",
        "- `GarageArea`, `OverallQual`\n",
        "\n",
        "    Intuitively, these features seem to be non-redundant so we will not discard either of them. \n",
        "\n",
        "- `YearBuilt`, `OverallQual`\n",
        "\n",
        "    Intuitively, these features seem to be non-redundant, so we will not discard either of them.\n",
        "\n",
        "- `YearBuilt`, `GarageCars`\n",
        "\n",
        "    `GarageCars` is already discarded based on the high-level analysis, so not action required.\n",
        "\n",
        "- `HalfBath`, `2ndFlrSF` \n",
        "\n",
        "    The correlation here is somewhat high, $>=0.60$. We can try discarding `HalfBath` or discarding `2ndFlrSF` \n",
        "\n",
        "- `BedroomAbvGr`, `GrLivArea`\n",
        "\n",
        "    We could try discarding `BedroomAbvGr`, since `GrLivArea` has a stronger correlation with `SalePrice` and `GrLivArea` is a continuous variable.\n",
        "\n",
        "- `TotalBsmtSF`, `OverallQual`\n",
        "\n",
        "    We have already discarded `TotalBsmtSF`, so no action is required.\n",
        "\n",
        "- `GrLivArea`, X - The `GrLivArea` is highly correlated with `FullBath`, `GarageCars`, and `OverallQual`. \n",
        "\n",
        "    Perhaps consider dropping other features besides `GrLivArea` \n",
        "\n",
        "- `FullBath`, `OverallQual` \n",
        "\n",
        "    Perhaps consider dropping `FullBath`, since `FullBath` has a weaker correlation with `SalePrice` and `FullBath` is correlated with other features such as `GrLivArea` already. \n",
        "\n",
        "- `GarageCars`, X\n",
        "\n",
        "    We are alreay dropping `GarageCars`, so no action is required here.\n",
        "\n",
        "\n",
        "In summary, there appears to be an interconnected relationship between `YearBuilt`, `OverallQual`, and `GarageArea` \n",
        "\n",
        "\n",
        "- **[ENG]** - Try discarding `BsmtFullBath`\n",
        "- **[ENG]** - Try discarding `TotRmsAbvGrd` and keeping the individual above-grade room counts. Compare this to just keeping `TotRmsAbvGrd` to determine if there is any performance boost. \n",
        "- **[ENG]** - Try discarding `GrLivArea` or `1stFlrSF` or keeping both to see if there is any performance boost.\n",
        "- **[ENG]** - Try discarding `HalfBath`. If we are preprocessing in an alternate way, where we discard `2ndFlrSF`, then try to keep `HalfBath` \n",
        "- **[ENG]** - Try discarding either `BsmtUnfSF` or `BsmtFinSF1` or keeping both.\n",
        "- **[ENG]** - Try discarding `BedroomAbvGr` over `GrLivArea`\n",
        "- **[ENG]** - Consider dropping `FullBath` and `OverallQual`, which are correlated with `GrLivArea`. Note that `FullBath` is correlated with `OverallQual`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMcK3faqYDnV"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(high_corrmat, ax=ax, square=True, annot=True, vmin=-1, vmax=1)\n",
        "ax.set_title(\"Features with at least 1 high correlation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSfNscp5UOAh"
      },
      "source": [
        "From the high correlation matrix, we see the following high correlation relationships between predictors:\n",
        "- `GarageYrBlt`, `YearBuilt` - Earlier, in the bar chart visualization, we saw that these two features have the same distribution and speculated that most houses are built alongside their garage. We see that the correlation value does not disprove this hypothesis. We can remove `GarageYrBlt` to reduce multicollinearity.\n",
        "- `GrLivArea`, `TotRmsAbvGrd` - According to the data documentation, `GrLivArea` represents \"Above grade (ground) living area square feet\" and `TotRmsAbvGrd` represents \"Total rooms above grade (does not include bathrooms)\". Thus, the two features are almost equivalent, which explains their high correlation. We discard `TotRmsAbvGrd`, because a continuous variable may be more suitable for linear regression. \n",
        "- `1stFlrSF`, `TotalBsmtSF` - This correlation is rather **surprising**. It appears that the surface area of a basement is correlated with the surface area of the first floor. Yet, `1stFlr` and `2ndFlrSF` has a correlation value of only -0.21. We can discard `TotalBsmtSF` to reduce multicollinearity for linear regression.\n",
        "- `GarageArea`, `GarageCars` - This correlation is to be expected. A garage that accomodates cars obviously requires more surface area. Interestingly, the correlation is very high, 0.9. This could be explained by some kind of construction standard, e.g., for every car we want to accomodate, add 900 sq ft to the garage. We discard `GarageCars` to reduce multicollinearity and since continuous variables are easier to work with. \n",
        "- `2ndFlrSF`, `GrLivArea` - Interestingly, `GrLivArea` has a stronger correlation with `2ndFlrSF` compared to `1stFlrSF`. We could discard `2ndFlrSF`. We do not discard `GrLivArea`, because it has a higher correlation with the target, `SalePrice`\n",
        "\n",
        "- **[ENG]** - Discard `GarageYrBlt`\n",
        "- **[ENG]** - Discard `TotRmsAbvGrd`\n",
        "- **[ENG]** - Discard `TotalBsmtSF`\n",
        "- **[ENG]** - Discard `GarageCars`\n",
        "- **[ENG]** - Discard `2ndFlrSF`\n",
        "- **[TASK]** - For each pair above, plot a scatterplot to illustrate their relationship, because they might not be in a linear relationship. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFQc_wICcBRx"
      },
      "source": [
        "predictors_ranking = corrmat['SalePrice'].sort_values(ascending=False)\n",
        "predictors_ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGatGwr1cJAQ"
      },
      "source": [
        "As shown above, `OverallQual`, `GrLivArea` have a high correlation\n",
        "\n",
        "- **[TASK]** - Check the relationship between `SalePrice` and each of the features above.\n",
        "- **[ENG]** - Include `GrLivArea`, `OverallQual`, `GarageArea`, `1stFlrSF`, \n",
        "`YearBuilt` all as predictors.\n",
        "- **[ENG]** - Run experiments using `FullBath`, `YearRemodAdd` as features\n",
        "- **[ENG]** - Run experiments using `MasVnrArea`, `Fireplaces`, `BsmtFinSF1`, `LotFrontage`, `2ndFlrSF`, `OpenPorchSF`, `WoodDeckSF` as features.\n",
        "- **[ENG]** - Run experiments using features with absolute correlation $<=0.30$ as predictors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUTdB8an0vuj"
      },
      "source": [
        "# Scatterplots between predictor values\n",
        "def plot_scatter(ax, feat):\n",
        "  subax = sns.scatterplot(ax=ax, data=combined_train, x=feat[0], y=feat[1])\n",
        "  return subax\n",
        "\n",
        "high_corr_pairs = [['1stFlrSF', 'TotalBsmtSF'], ['GrLivArea', 'TotRmsAbvGrd'], \n",
        "                   ['GarageYrBlt', 'YearBuilt'], ['GarageCars', 'GarageArea'],\n",
        "                   ['2ndFlrSF', 'GrLivArea']]\n",
        "med_corr_pairs = [['YearRemodAdd', 'YearBuilt'], ['BsmtFullBath', 'BsmtFinSF1'],\n",
        "                  ['BsmtUnfSF', 'BsmtFinSF1'], ['1stFlrSF', 'GrLivArea'],\n",
        "                  ['GarageArea', 'OverallQual'], ['YearBuilt', 'OverallQual'],\n",
        "                  ['HalfBath', '2ndFlrSF'], ['BedroomAbvGr', 'GrLivArea'],\n",
        "                  ['FullBath', 'OverallQual'], ['GrLivArea', 'FullBath'], \n",
        "                  ['GrLivArea', 'GarageCars'], ['GrLivArea', 'OverallQual']]\n",
        "\n",
        "pairs = high_corr_pairs + med_corr_pairs\n",
        "\n",
        "plot_grid(plot_scatter, pairs, nrows=5, ncols=4, figlen=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvV3QlJc0rYS"
      },
      "source": [
        "In the scatterplots above, we have chosen to plot highly- and moderately-correlated features, as plotting all 36 x 36 scatterplots using `sns.pairplot` has a long runtime.\n",
        "\n",
        "We make the following observations:\n",
        "- `1stFlrSF`, `TotalBsmtSF` - There are appear to be three populations in this scatterplot: houses with no basement, houses where `1stFlrSF == TotalBsmtSF`, and all other houses. There are a few outliers: one where the basement is far larger than the first floor, and one where the first floor is far larger than the basement. \n",
        "- `GrLivArea`, `TotRmsAbvGrd` - There is an obivous, positive linear trend. There appear to be two outliers at the far right that do not obey the trend. \n",
        "- `GarageYrBuilt`, `YearBuilt` - Illustrates two populations: one where the garage was built alongside the house, one where the garage was built after the house. There is also a third population of houses that do not have garages. \n",
        "- `GarageCars`, `GarageArea` - Illustrates an obvious, positive linear trend.\n",
        "- `2ndFlrSF`, `GrLivArea` - There are two populations: (1) houses without a second floor, (2) houses with a second floor. In the latter case, there is a clear linear trend.\n",
        "- `YearRemodAdd`, `YearBuilt` - This illustrates a fact that was listed in the data documentation: for `YearRemodAdd`, it is \"same as construction date [`YearBuilt`] if no remodeling or additions\". There are two populations: (1) houses that were never remodelled, (2) houses that were remodelled. The later population shows no trend, which is to be expected. \n",
        "- `BsmtUnfSF`, `BsmtFinSF1` - This scatterplot suggests that there is no real relationship between the two features. The moderate correlation level might be a fluke. Hence, there is no need to drop either feature, since they have little redundancy. \n",
        "- `1stFlrSf`, `GrLivArea` - This scatterplot reveals a fact that should have been obvious, in hindsight. If a house has a single floor above ground, then `1stFlrSF` is exactly equal to `GrLivArea` \n",
        "- `GarageArea`, `OverallQual` - This has a clear, positive linear trend. One hypothesis to explain this trend, is that the two features are linked to a third variable, the general \"value\" of a property, that is, rich estates or mansions have large garages and high quality, whereas cheaper housing tends to have less garages and are lower quality. \n",
        "- `YearBuilt`, `OverallQual` - This relationship makes intuitive sense. New housing has yet to be damaged by age.\n",
        "- The rest of the graphs show obvious relationships like `YearBuilt`, `OverallQual`\n",
        "\n",
        "- **[TASK]** - For pairs of features that show an obvious line of data points, we can construct new features by taking the difference between the pair. \n",
        "- **[TASK]** - Try to manipulate `1stFlrSF`, `2ndFlrSF`, and `GrLivArea` to find their relationship. Perhaps consider a 3D plot. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AuyxNw4T0SH"
      },
      "source": [
        "#### Categorical vs. Categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x83JItyY6xGl"
      },
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def make_contigency_table(data, x, y):\n",
        "  df = []\n",
        "  uniq_xs = np.unique(data[x].dropna(axis=0))\n",
        "  uniq_ys = np.unique(data[y].dropna(axis=0))\n",
        "\n",
        "  for target in uniq_ys:\n",
        "    row = []\n",
        "    for pred in uniq_xs:\n",
        "      X_with_pred = data[data[x] == pred]                     # get all records in `data` with the specific value for the predictor\n",
        "      X_with_target = X_with_pred[X_with_pred[y] == target]   # get all records in `X_with_pred` with the specified predictor AND target\n",
        "      cell = len(X_with_target)                               # now count the number of records with `x=pred`, `y=target`\n",
        "      row.append(cell)\n",
        "    df.append(row)\n",
        "\n",
        "  return pd.DataFrame(df).transpose()\n",
        "\n",
        "def cramers_v(cont_table):\n",
        "  chi2 = chi2_contingency(cont_table, correction=False)[0]\n",
        "  N = cont_table.to_numpy().sum()\n",
        "  k = np.min(cont_table.shape) - 1\n",
        "  V = np.sqrt((chi2 / N) / k)\n",
        "  return V\n",
        "\n",
        "def cat_corrmat(data, object_cols):\n",
        "  matrix = []\n",
        "  for row_feat in object_cols:\n",
        "    row = []\n",
        "    for col_feat in object_cols:\n",
        "      try:\n",
        "        tbl = make_contigency_table(data, row_feat, col_feat)\n",
        "        v = cramers_v(tbl)\n",
        "        row.append(v)\n",
        "      except ValueError:\n",
        "        row.append(np.nan) # fill the cell with some error value \n",
        "    matrix.append(row)\n",
        "  return pd.DataFrame(matrix, index=object_cols, columns=object_cols)\n",
        "\n",
        "def plot_cat_corrmatrix(corrmat, object_cols):\n",
        "  fig, ax = plt.subplots(figsize=(30, 30))\n",
        "  subax = sns.heatmap(corrmat, ax=ax, annot=True)\n",
        "\n",
        "catmatrix = cat_corrmat(combined_train_with_absent, object_cols)\n",
        "plot_cat_corrmatrix(catmatrix, object_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB2Jtf0uUrd1"
      },
      "source": [
        "low_cat_feats, med_cat_feats, high_cat_feats, _ = get_feat_corr_hierarchy(catmatrix)\n",
        "\n",
        "print(low_cat_feats)\n",
        "print(med_cat_feats)\n",
        "print(high_cat_feats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xzqaf-EqLO-"
      },
      "source": [
        "def get_submatrix(corrmat, to_keep):\n",
        "  rows_to_keep = corrmat.index.isin(to_keep)\n",
        "  return corrmat[to_keep][rows_to_keep]\n",
        "\n",
        "low_cat_corrmat = get_submatrix(catmatrix, low_cat_feats)\n",
        "med_cat_corrmat = get_submatrix(catmatrix, med_cat_feats)\n",
        "high_cat_corrmat = get_submatrix(catmatrix, high_cat_feats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soUoS5cS0Llk"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(high_cat_corrmat, ax=ax, square=True, annot=True, vmin=0, vmax=1)\n",
        "ax.set_title(\"Features with at least 1 high correlation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dATGH4D7Bvfa"
      },
      "source": [
        "The follow features are highly correlated and are not independent.\n",
        "\n",
        "1. `Exterior1st`, `Exterior2nd`\n",
        "2. `GarageQual`, `GarageCond`\n",
        "\n",
        "- **[ENG]** - Discard `Exterior2nd`\n",
        "- **[ENG]** - Discard `GarageQual` or `GarageCond`. Try to measure the correlation between `SalePrice` and the two features, then discard the one with the lowest correlation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZvTuaUp0GPX"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(med_cat_corrmat, ax=ax, square=True, annot=True, vmin=0, vmax=1)\n",
        "ax.set_title(\"Features with at least 1 medium correlation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jInoQCYc_HjX"
      },
      "source": [
        "The following features have medium correlation\n",
        "\n",
        "- `Neighborhood`, `MSZoning` - These two features are somewhat redundant, since zoning might not vary much within the same neighborhood. \n",
        "- `KitchenQual`, `ExterQual` - These variables appear to be non-redundant. One is the quality of the kitchen, one is the quality of the exterior. No action here.\n",
        "\n",
        "There are three \"clusters\" of highly related features.\n",
        "\n",
        "The first cluster contains the categorical features for the basement, which all have medium correlations. This subset of features might contain redundant information between them.\n",
        "\n",
        "- `BsmtQual`, `Foundation` - These features do not appear to be redundant information, so we might keep both of them. \n",
        "- `BsmtCond`, `BsmtQual` - We should consider discarding one of these.\n",
        "- `BsmtExposure`, `BsmtQual` - Perhaps consider dropping `BsmtExposure` since `BsmtQual` can be interpreted as a continuous feature. \n",
        "- `BsmtExposure`, `BsmtCond` - Perhaps consider dropping `BsmtExposure` since `BsmtCond` can be interpreted as a continuous feature.\n",
        "- `BsmtFinType1`, `BsmtQual` - Perhaps consider dropping `BsmtFinType`\n",
        "- `BsmtFinType1`, `BsmtCond` - Perhaps consider dropping `BsmtFinType`\n",
        "- `BsmtFinType1`, `BsmtExposure` - Perhaps consider dropping `BsmtFinType`\n",
        "- `BsmtFinType2`, X - Same as `BsmtFinType`\n",
        "\n",
        "Secondly, the categorical features for the garage all have medium correlations.\n",
        "\n",
        "- `GarageFinish`, `GarageType` - Perhaps discard `GarageType` in favour of `GarageFinish`, because finish can be interpreted as ordinal, whereas `GarageType` cannot.\n",
        "- `GarageQual`, `GarageFinish` - Perhaps discard `GarageFinish`? \n",
        "- `GarageCond`, `GarageFinish`\n",
        "\n",
        "Lastly, the there appears to be another cluster involving the following features, where any pairs of 2 features have moderate or to close to moderate values for Cramer's V\n",
        "\n",
        "- `Exterior1st`\n",
        "- `Exterior2nd`\n",
        "- `ExteriorQual`\n",
        "- `Foundation`\n",
        "- `BsmtQual`\n",
        "\n",
        "Interestingly, `Neighborhood` has low to moderate correlations with several features. This is in accordance with conventional wisdom which implies that neighborhood is a critical factor in finding good housing, as it can determine overall house quality and price. \n",
        "\n",
        "- **[ENG]** - Empirically check if discarding `Neighborhood` or `MSZoning` improves performance\n",
        "- **[ENG]** - Discard `BsmtCond` or `BsmtQual`, depending on which has less correlation with `SalePrice`\n",
        "- **[ENG]** - Try discarding `BsmtExposure`, `BsmtFinType1`, and `BsmtFinType2`, as they are nominal variables, whereas `BsmtQual` and `BsmtCond` are continuous variables that capture some of the information. \n",
        "- **[ENG]** - Empirically check if discarding `GarageFinish` and/or `GarageType` in favour of `GarageQual` leads to performance boosts.\n",
        "- **[ENG]** - Try manipulating the basement and garage categorical variables to remove redundancy and improve performance. Create a preprocessor that empirically checks every non-empty subset of the collinear features, then tests to see which one yields the best performance. \n",
        "- **[ENG]** - For the first model, try building a simple model using only numerical and ordinal features. Then, add nominal features as required to boost performance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGPKyKS0L0V"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(low_cat_corrmat, ax=ax, square=True, annot=True, vmin=0, vmax=1)\n",
        "ax.set_title(\"Features with at least 1 low correlation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HxVVF3fCK2b"
      },
      "source": [
        "We skip the analysis of the low correlation features for now, until we need additional performance boosts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8xO2FiwT2YN"
      },
      "source": [
        "#### Numeric vs. Categorical\n",
        "\n",
        "To exhaustively assess numerical vs. categorical relationships, we would need to plot distributions for every pair of 1 numerical and 1 categorical feature. However, there are at least 30 numerical and 30 categorical features, necessitating at least 900 plots. This would take too long to run.\n",
        "\n",
        "Instead, we first compute the matrix of Kruskall-Wallis H test values. Then, for any entries that are particularly high, we plot their distributions to confirm that the continuous and numeric feature pair meets the H test's assumptions.\n",
        "\n",
        "The advantage of this approach is that the notebook will take less time to run.\n",
        "\n",
        "The disadvantage of this approach is that we may miss false negatives, that is, feature pairs where the H test assumption is violated and the two features in fact have a high association, but the corresponding H-test value is not high enough, so we wouldn't think to check this pair.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8hLG0CsrD5s"
      },
      "source": [
        "from scipy.stats import kruskal\n",
        "\n",
        "def partition_by_cat(data, cat_feat, cont_feat):\n",
        "  series = data[cat_feat]\n",
        "  group_labels = np.unique(series.dropna())\n",
        "  groups = []\n",
        "  for label in group_labels:\n",
        "    group = data[series == label][cont_feat]\n",
        "    groups.append(group)\n",
        "  return groups\n",
        "\n",
        "def kruskal_wallis_h(data, cat_feat, cont_feat):\n",
        "  groups = partition_by_cat(data, cat_feat, cont_feat)\n",
        "  k, p = kruskal(*groups, nan_policy='omit')\n",
        "  return k, p\n",
        "\n",
        "def kruskal_matrix(data, object_feats, numerical_feats, option='statistic',\n",
        "                   transpose = False):\n",
        "  rows = []\n",
        "  for ofeat in object_feats:\n",
        "    row = []\n",
        "    for nfeat in numerical_feats:\n",
        "      h, p = kruskal_wallis_h(data, ofeat, nfeat)\n",
        "\n",
        "      cell_val = None\n",
        "      if option == 'statistic':\n",
        "        cell_val = h\n",
        "      elif option == 'pvalue':\n",
        "        cell_val = p\n",
        "      else:\n",
        "        raise ValueError('incorrect value for option in `kruskal_matrix`')\n",
        "\n",
        "      row.append(cell_val)\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "  retVal = pd.DataFrame(rows, index=object_feats, columns=numerical_feats)\n",
        "\n",
        "  return retVal if not transpose else retVal.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjUCjz8-7Okk"
      },
      "source": [
        "\n",
        "\n",
        "obj_cols = object_cols + wrong_num_cols\n",
        "num_cols = list(set(numerical_cols) - set(wrong_num_cols)) + [target_col]\n",
        "kmatrix = kruskal_matrix(combined_train_with_absent, \n",
        "                         obj_cols,\n",
        "                         num_cols,\n",
        "                         option='pvalue',\n",
        "                         transpose=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(30, 30))\n",
        "sns.heatmap(- np.log(kmatrix), ax=ax, annot=True, vmin=0, vmax=2)\n",
        "ax.set_title(\"Heatmap of negative log p-values from Kruskall-Wallis H test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "---avaI5o1dT"
      },
      "source": [
        "In the heatmap above, any value of 2 or higher corresponds to a $p$-value of 0.01 or lower. We take this to mean there is strong evidence that the alternate hypothesis, that the numerical and continuous variables are dependent is true. \n",
        "\n",
        "Unfortunately, there are more statistically significant associations in the heatmap than originally anticipated. We will concentrate on the row with `SalePrice` and use the other rows as needed. According to the heatmap, there is strong statistical evidence of a relationship between almost every categorical feature and `SalePrice`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxOgCT1WqF_O"
      },
      "source": [
        "We now verify the assumptions for the Kruskall-Wallis H Test to check if the test gives valid results. To do this, given a categorical feature, we must check if the probability distribution of `SalePrice` is the same shape over all the values of the categorical feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEVYBk39qpX6"
      },
      "source": [
        "def plot_multidist(ax, feat): \n",
        "  return sns.histplot(combined_train_with_absent, ax=ax, x=target_col, hue=feat,\n",
        "                      fill=True)\n",
        "  \n",
        "plot_grid(plot_multidist, obj_cols, ncols=7, nrows=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4BIV6vPxT2C"
      },
      "source": [
        "Unfortunately, it appears that many plots show that the distributions do not have the same shape. For many plots, this is due to distortions caused by small group sizes making it difficult to obtain a reliable comparison. \n",
        "\n",
        "For a future EDA, we may consider randomly sampling each group so that we compare the probability distributions of similarly sized data-sets. We can use the resulting plot as an estimate of the \"true\" behaviour among groups in the overall population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCI0yngIqAEC"
      },
      "source": [
        "### Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgpBVY3G3lvt"
      },
      "source": [
        "#### Determine What a \"High-end\" Home Typically Looks Like\n",
        "\n",
        "- Refer back to the univariate analysis for \"luxury\" assets\n",
        "- Check if these features really correspond to \"luxury\" assets by checking the `SalePrice` of homes that have these assets (that is homes with boolean = 1 or continuous_var > 0) OR if these luxury assets are really undesirable or obsolete characteristics. \n",
        "\n",
        "- Try to identifying the outliers using the `outlier_threshold` variable then check feature for any patterns. If you see a pattern, check against non-outlier houses to see if that pattern is not present in the lower end houses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzj7FIj21N2e"
      },
      "source": [
        "series = combined_train_with_absent['SalePrice']\n",
        "outlier_homes = combined_train_with_absent[series >= outlier_threshold]\n",
        "outlier_homes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBo4bPXQ1pJn"
      },
      "source": [
        "outlier_homes[num_cols].describe() - combined_train_with_absent[num_cols].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDcI_he084wE"
      },
      "source": [
        "The outlier homes have:\n",
        "- `GrLivArea` - A greater amount of space on average.\n",
        "- `LotArea` - A significantly higher lot area on average.\n",
        "- `OverallQual` - A higher overall quality on average\n",
        "- `BsmtFinSF1` - High basement finished area \n",
        "- `GarageArea` - Higher garage area\n",
        "- `MasVnrArea` - Higher masonry veneer area\n",
        "- `YearBuilt` - The average year it was built is 25 years later. \n",
        "\n",
        "In general, the houses are newer and have higher quality overall. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBvnoxkC1sJu"
      },
      "source": [
        "plot_grid(plot_catcount(outlier_homes), obj_cols, nrows=7, ncols=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YNg2WZQ4QRE"
      },
      "source": [
        "We make the following observations about the outlier homes:\n",
        "- `MSZoning` - They must be either residential low density or resdiential medium density.\n",
        "- `Street` - Must be paved\n",
        "- `Alley` - Must be absent\n",
        "- `LotShape` - There are more houses with slightly irregular shapes, than normal shapes. The proportion of moderately and highly irregular shapes is also higher than the overall dataset.\n",
        "- `AllPub` - These houses all have their utilities.\n",
        "- `Neighborhood` - The housing belong to `Crawfor`, `NridgHt`, `NoRidge`, `Timber`, `StoneBr`, `Somerst`, `OldTown`, `CollgCr`, `Gilbert`. The majority belong in `NridgHt`\n",
        "- `MasVnrType` - The most to least frequent masonry veneer type is `Stone`, `BrkFace`, and `None`. This is the reverse order of `MasVnrType` in the overall dataset. \n",
        "- `ExterQual` - A high proportion of houses have good or better exterior quality. None have lower than average.\n",
        "- `ExterCond` - Roughly the same proportions as the overall dataset, except condition is at least good.\n",
        "- `Foundation` - Foundation is dominated by concrete, in contrast to the overall dataset, which is either concrete or cinderblock. \n",
        "- `BsmtQual` - Basement quality is at least average. Most of the basements have excellent quality.\n",
        "- `BsmtBond` - Basement condition is at least average.\n",
        "- `BsmtExposure` - Basement exposure is good on average. The `Gd` category makes a significantly higher proportion of the outliers, than in the overall dataset.\n",
        "- `BsmtFinType` - A large proportion of basements are good living quarters. \n",
        "- `HeatingQC` - Most are excellent quality. All are at least average.\n",
        "- `CentralAir` - All have a centralized system\n",
        "- `Electrical` - All have `SBrkr` \n",
        "- `KitchenQual` - Many have excellent quality\n",
        "- `HeatingQC` - Most have excellent quality\n",
        "- `FireplaceQu` - All have the houses have fireplaces. They are all at least average quality. A large proportion are in excellent quality.\n",
        "- `GarageType` - All of the houses have garages. Most of them are attached. \n",
        "- `GarageFinish` - Most of the houses have finished garages. \n",
        "- `GarageQual` - At least average\n",
        "- `PavedDrive` - All of the houses have paved driveways. \n",
        "- `PoolQC`, `Fence`, `MiscFeature` - These are all absenta nd clearly do not contribute to the outlier's high price. \n",
        "- `SaleType` - An unusually high proportion of sales are \"New\" (home just constructed and sold) and \"ConLI\" (contrast low interest), and the rest are \"WD\". \n",
        "- `SaleCondition` - An unusually high proportion of sales are \"Partial\" (meaning the home was not completed when last assessed), the rest are \"Normal\" or \"Alloca\"\n",
        "- `MSSubClass` - The class is predominantly 20 (\"1-STORY 1946 & NEWER ALL STYLES\") or 60 (\"2-story 1946 & NEWER\"). T\n",
        "\n",
        "To summarize the outlier houses, they generally have at least average quality in nearly every aspect. They tend to have a basement that serves as good living quarters. They belong to a specific set of neighborhoods. They often have \"conventional\" design that is the most frequent value in the overall dataset, e.g, most housing have centralized air conditioning, so the outliers all have centralized air condition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ3Wd1LZK6oI"
      },
      "source": [
        "### Model Assumptions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMui-97H_OxQ"
      },
      "source": [
        "#### Linear Regression\n",
        "\n",
        "For simplicity, we will perform the checks of linear regression model assumptions against the numeric predictors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlzJ_sxEs_kS"
      },
      "source": [
        "def make_assume_pipeline():\n",
        "    return Pipeline(steps=[('preprocessor', numerical_transformer),\n",
        "                           ('model', LinearRegression())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfnUXNXqDkcw"
      },
      "source": [
        "##### Linear Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k1W5WozDmvj"
      },
      "source": [
        "def plot_for_linearity(X_train, X_valid, y_train, y_valid, numerical_cols):\n",
        "  pipe_assume = make_assume_pipeline()\n",
        "  pipe_assume.fit(X_train[numerical_cols], y_train)\n",
        "\n",
        "  y_preds = pipe_assume.predict(X_valid[numerical_cols])\n",
        "\n",
        "  residuals = y_valid - y_preds\n",
        "\n",
        "  subax = sns.regplot(x=y_preds, y=residuals)\n",
        "  subax.set_xlabel('Fitted Response')\n",
        "  subax.set_ylabel('Residuals')\n",
        "  subax.axhline(0, color='r')\n",
        "\n",
        "  return residuals\n",
        "\n",
        "plot_for_linearity(X_train, X_valid, y_train, y_valid, numerical_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3j3qovoGaIm"
      },
      "source": [
        "Linearity is clearly being violated. The data points appear to \"dipping\" down to a minimum around 200,000, then curving upwards. This appears to be a quadratic relationship between the fitted response and residuals.\n",
        "\n",
        "We will attempt to transform the features to ensure linearity between `SalePrice` and each feature. Ideally, this will correct the residuals vs. fit plot to follow linearity. \n",
        "\n",
        "In fact, using a simple log transformation (rather than a square-root), we are able to satisfy linearity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5pPHrg_JuCK"
      },
      "source": [
        "plot_for_linearity(X_train, X_valid, np.log(y_train), np.log(y_valid), numerical_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aclWSFjEgjdV"
      },
      "source": [
        "There appears to be two outliers with a very low residual value. If we could ignore these outliers, then the plot will satisfy the linearity assumption better.\n",
        "\n",
        "- **[ENG]** - Fit to the logarithm of `SalePrice` to satisfy linearity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ9d1Cg4gc8E"
      },
      "source": [
        "residuals = plot_for_linearity(X_train, X_valid, np.log(y_train), np.log(y_valid), numerical_cols)\n",
        "plt.ylim([-1, 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNK8aSY34obX"
      },
      "source": [
        "# Determine the ID of the residual points\n",
        "residuals[residuals <= -0.6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1DJrf-A4w4n"
      },
      "source": [
        "_y_valid = y_valid.copy()[(y_valid.index != 633) & (y_valid.index != 1299)]\n",
        "_X_valid = X_valid.copy()[(X_valid.index != 633) & (X_valid.index != 1299)]\n",
        "residuals = plot_for_linearity(X_train, _X_valid, \n",
        "                               np.log(y_train), np.log(_y_valid), \n",
        "                               numerical_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKBotMGQ5Rmm"
      },
      "source": [
        "As can be seen above, the plot appears to have a roughly random spread of data points around $y=0$, suggesting linearity.\n",
        "\n",
        "However, we needed to discard outliers to achieve this, this could introduce systemic flaws to the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJXZVfn0yrm"
      },
      "source": [
        "##### Homoscedasticity\n",
        "In the check for \"Linear Function\", we see that the residuals roughly lie around a horizontal band around $y=0$, roughly in range $[-0.4,0.4]$. This indicates that so long as we take the logarithm of `SalePrice`, the assumption of homoscedasticity is satisfied.\n",
        "\n",
        "- **[ENG]** - Take the logarithm of `SalePrice` to satisfy homoscedasticity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCJ0OlzCg0GI"
      },
      "source": [
        "##### No Autocorrelation\n",
        "To evaluate auto-correlation, we can check if the residuals are auto-correlated when ordered by `YearBuilt`, `YearModAdd`, or `YrSold` and `MoSold`. This is not a perfect analysis, but we can eliminate some obvious possibilities for auto-correlation.\n",
        "\n",
        "We ignore `GarageYrBlt` since we know that this is highly correlated with `YearBuilt` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obKB2ISevbcp"
      },
      "source": [
        "X_train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwvQhRgBm9QH"
      },
      "source": [
        "def compute_residuals(X_train, X_valid, y_train, y_valid, cols):\n",
        "  pipe = make_assume_pipeline()\n",
        "\n",
        "  pipe.fit(X_train[cols], y_train)\n",
        "  y_preds = pipe.predict(X_valid[cols])\n",
        "\n",
        "  residuals_train = X_valid[cols].copy()\n",
        "  residuals_train['Residuals'] = y_preds - y_valid\n",
        "\n",
        "  return residuals_train\n",
        "\n",
        "def compute_ordered_residuals(X_train, X_valid, y_train, y_valid, by, cols):\n",
        "  residuals_train = compute_residuals(X_train, X_valid, y_train, y_valid,\n",
        "                                      cols)\n",
        "\n",
        "  return residuals_train[[by, 'Residuals']].sort_values(by=[by])\n",
        "\n",
        "_num_cols = list(set(numerical_cols) - set(wrong_num_cols))\n",
        "\n",
        "train_by_built  = compute_ordered_residuals(X_train, X_valid, \n",
        "                                            np.log(y_train), np.log(y_valid), \n",
        "                                            'YearBuilt', _num_cols)\n",
        "train_by_mod    = compute_ordered_residuals(X_train, X_valid, \n",
        "                                            np.log(y_train), np.log(y_valid), \n",
        "                                            'YearRemodAdd', _num_cols)\n",
        "\n",
        "X_train_time = X_train.copy()\n",
        "X_train_time['TimeSold'] = X_train_time['YrSold'] + X_train_time['MoSold'] / 12\n",
        "X_valid_time = X_valid.copy()\n",
        "X_valid_time['TimeSold'] = X_valid_time['YrSold'] + X_valid_time['MoSold'] / 12\n",
        "\n",
        "train_by_sold   = compute_ordered_residuals(X_train_time, X_valid_time,\n",
        "                                            np.log(y_train), np.log(y_valid),\n",
        "                                            'TimeSold', _num_cols + ['TimeSold'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3mfCF4xwRbx"
      },
      "source": [
        "from statsmodels.stats.stattools import durbin_watson\n",
        "\n",
        "dw_built  = durbin_watson(train_by_built['Residuals'])\n",
        "dw_mod    = durbin_watson(train_by_mod['Residuals'])\n",
        "dw_sold   = durbin_watson(train_by_sold['Residuals'])\n",
        "\n",
        "print(\"Durbin-Watson for sorted by by YearBuilt: %f\" % dw_built)\n",
        "print(\"Durbin-Watson for sorted by by YearRemodAdd: %f\" % dw_mod)\n",
        "print(\"Durbin-Watson for sorted by by TimeSold: %f\" % dw_sold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z95azVKCwtQ-"
      },
      "source": [
        "Since the test value is very close to 2, this demonstrates that there is no auto-correlation in the residuals, when ordered by the year built, year remodifications were add, or the year the house was sold.\n",
        "\n",
        "Hence, this assumption appears to be satisfied. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4oYqHRZxNgj"
      },
      "source": [
        "##### Normally-Distributed Residuals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3xsqv5kxQB6"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "resid_train = compute_residuals(X_train, X_valid, \n",
        "                                np.log(y_train), np.log(y_valid),\n",
        "                                _num_cols)\n",
        "sm.qqplot(resid_train['Residuals'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjVM90dtysIc"
      },
      "source": [
        "sm.qqplot(resid_train['Residuals'])\n",
        "plt.ylim([-0.5, 0.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPP-0NAOyxxB"
      },
      "source": [
        "The QQ-plot shows a roughly straight line, however, there are are two outliers with an extremely high residual. \n",
        "\n",
        "We will assume that normality is satisfied and investigate the outlier points (which is in the validation set) after the machine learning models are developed.\n",
        "\n",
        "- **[TASK]** - Investigate outlier points in qqplot above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swi2pydEzst7"
      },
      "source": [
        "##### No Multicollinearity\n",
        "\n",
        "As shown in the multivariate analysis, \"Numeric vs. Numeric\", there are a number of multicollinear features such as `YearBuilt` and `GarageYrBuilt`. \n",
        "\n",
        "Hence, the original raw dataset violates the multicollinearity assumptions. We can resolve this issue by discarding features with high multicollinearity. For more information on which features are discarded, see the \"Numeric vs. Numeric\" analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3m4-MRM0CIl"
      },
      "source": [
        "##### No Outliers\n",
        "In the univariate analysis, there were a number of points that were greater than `threshold = q75 + 1.5 * IQR`. This indicates the presence of possible outliers, and so, this assumption is likely violated. \n",
        "\n",
        "Hence, either we can discard the outliers, which might introduce some systemic error if there is a consistent reason for the outliers' existence, or use a different model that is less sensitive to outliers. \n",
        "\n",
        "- **[ENG]** - Try fitting the linear regression model with and without the outlier points, to see if that improves performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elWE6xP5_M_s"
      },
      "source": [
        "#### Random Forest, Gradient Boosting, ANNs\n",
        "\n",
        "There are no assumptions to check here, besides the standard assumptions: (1) sampling is representative, (2) values are independent and identically distributed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_gXE3GPk3_B"
      },
      "source": [
        "## Preprocessing Features\n",
        "\n",
        "Based on the results of the EDA, we must perform the following steps for all models\n",
        "\n",
        "- **[ENG]** - Construct the initial model using `GrLivArea`, `OverallQual`,`GarageArea`, `1stFlrSF`, `YearBuilt` all as predictors.\n",
        "- **[ENG]** - Impute `LotFrontage` to 0, median, or mean. \n",
        "- **[ENG]** - Discard `GarageYrBlt`. There is no reasonable imputation value and it is correlated with `YearBuilt`, so the information is redundant.\n",
        "- **[ENG]** - Impute `Electrical` to be the most frequent value.\n",
        "- **[ENG]** - Change `MSSubClass` into a categorical feature. \n",
        "- **[ENG]** - Try: (1) impute MasVnrArea using the median/mean and MasVnrType with the most frequent observation, (2) first impute MasVnrType with the most frequent observation, then impute MasVnrArea with the average MasVnrArea for the most frequent MasVnrType.\n",
        "- **[ENG]** - Drop the record where `Stone` has 0 `MasVnrArea` (id 1242). This might be erroneous record.\n",
        "- **[ENG]** - Replace \"NA\" with \"Absent\" for some of the features.\n",
        "\n",
        "We will perform the following steps primarily for linear regression. Although, we may experiment with using these steps for other models. \n",
        "\n",
        "- **[ENG]** - Discard `GarageYrBlt`\n",
        "- **[ENG]** - Discard `TotRmsAbvGrd`\n",
        "- **[ENG]** - Discard `TotalBsmtSF`\n",
        "- **[ENG]** - Discard `GarageCars`\n",
        "- **[ENG]** - Discard `2ndFlrSF`\n",
        "- **[ENG]** - Fit to the logarithm of `SalePrice` to satisfy linearity.\n",
        "\n",
        "We can try the following additional steps on the models, especially linear regression, to see if it improves performance. \n",
        "\n",
        "- **[ENG]** - Drop `SalePrice` outliers to see if that improves model performance.\n",
        "- **[ENG]** - Try discarding `BsmtFullBath`\n",
        "- **[ENG]** - Try discarding `TotRmsAbvGrd` and keeping the individual above-grade room counts. Compare this to just keeping `TotRmsAbvGrd `to determine if there is any performance boost.\n",
        "- **[ENG]** - Try discarding `GrLivArea` or `1stFlrSF` or keeping both to see if there is any performance boost.\n",
        "- **[ENG]** - Try discarding `HalfBath`. If we are preprocessing in an alternate way, where we discard `2ndFlrSF`, then try to keep `HalfBath`\n",
        "- **[ENG]** - Try discarding either `BsmtUnfSF` or `BsmtFinSF1` or keeping both.\n",
        "- **[ENG]** - Try discarding `BedroomAbvGr` over `GrLivArea`\n",
        "- **[ENG]** - Consider dropping `FullBath` and `OverallQual`, which are correlated with `GrLivArea`. Note that `FullBath` is correlated with `OverallQual`\n",
        "- **[ENG]** - Run experiments using `FullBath`, `YearRemodAdd` as features\n",
        "- **[ENG]** - Run experiments using `MasVnrArea`, `Fireplaces`, `BsmtFinSF1`, `LotFrontage`, `2ndFlrSF`, `OpenPorchSF`, `WoodDeckSF` as features.\n",
        "- **[ENG]** - Run experiments using features with absolute correlation $<=0.30$ as predictors.\n",
        "\n",
        "If additional boosts in performance are necessary, then we might consider adding the categorical features. These features are somewhat undesirable  for linear regression, because they are not continuous features which would \"play\" better with the model. These might be useful in other models, however. \n",
        "\n",
        "- **[ENG]** - Empirically check if discarding `Neighborhood` or `MSZoning` improves performance\n",
        "- **[ENG]** - Discard `BsmtCond` or `BsmtQual`, depending on which has less correlation with `SalePrice`\n",
        "- **[ENG]** - Try discarding `BsmtExposure`, `BsmtFinType1`, and `BsmtFinType2`, as they are nominal variables, whereas `BsmtQual` and `BsmtCond` are continuous variables that capture some of the information. \n",
        "- **[ENG]** - Empirically check if discarding `GarageFinish` and/or `GarageType` in favour of `GarageQual` leads to performance boosts.\n",
        "- **[ENG]** - Try manipulating the basement and garage categorical variables to remove redundancy and improve performance. Create a preprocessor that empirically checks every non-empty subset of the collinear features, then tests to see which one yields the best performance. \n",
        "\n",
        "Some additional suggestions for experiments:\n",
        "\n",
        "- **[ENG]** - Try computing the number of years in-between YearBuilt and YearSold as a feature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etB58rScmfjF"
      },
      "source": [
        "X['MSSubClass']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAL5V06BmhJq"
      },
      "source": [
        "X['MSSubClass'] = X['MSSubClass'].astype('object')\n",
        "X[cols_with_NA_category] = X[cols_with_NA_category].fillna(NA_category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDqv1FELmiDE"
      },
      "source": [
        "# Recompute `X_train`, `X_valid` (using the same random seed, of course).\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=global_valid_size, random_state=global_random_state\n",
        ")\n",
        "\n",
        "# Recompute the numerical and object columns\n",
        "mask = X.dtypes == 'object'\n",
        "object_cols = list(mask[mask].index)\n",
        "numerical_cols = list(set(X.columns) - set(object_cols))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxPE4l2KoagZ"
      },
      "source": [
        "numerical_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt9AZuGOobF2"
      },
      "source": [
        "object_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZYXzZLJnpjW"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOLd1njCnqUj"
      },
      "source": [
        "X_valid"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}